%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% From a template maintained at https://github.com/jamesrobertlloyd/cbl-tikz-poster
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[portrait,a0b,final,a4resizeable]{include/a0poster}


\usepackage{multicol}
\usepackage{color}
\usepackage{morefloats}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{include/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{blue}{rgb}{0,0,1}
\usepackage{dsfont}
\usepackage[margin=0.5in]{geometry}
\usepackage{fp}

\input{include/jlposter.tex}

\usepackage{include/preamble}


% Custom notation
\newcommand{\fdeep}{\vf^{(1:L)}}
\newcommand{\flast}{\vf^{(L)}}
\newcommand{\Jx}{J_{\vx \rightarrow \vy}}
\newcommand{\Jxx}{J_{\vx \rightarrow \vy}(\vx)}
\newcommand{\Jy}{J_{\vy \rightarrow \vx}}
\newcommand{\Jyy}{J_{\vy \rightarrow \vx}(\vy)}
\newcommand{\detJyy}{ \left| J_{\vy \rightarrow \vx}(\vy) \right|}

\newcommand\transpose{{\textrm{\tiny{\sf{T}}}}}
\newcommand{\note}[1]{}
\newcommand{\hlinespace}{~\vspace*{-0.15cm}~\\\hline\\\vspace*{0.15cm}}
\newcommand{\embeddingletter}{g}
\newcommand{\bo}{{\sc bo}}
\newcommand{\agp}{Arc \gp}

\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{y}
\newcommand{\data} {\X, \y}
\newcommand{\x}{\mathbf{x}}
\newcommand{\f}{\mathit{f}}

\newcommand{\fx}{ f(\mathbf{x}) }
\newcommand{\U}{\mathcal{U}}
\newcommand{\E}{\mathbf{E}}

\def\layersep{10cm}
\def\nodesep{6cm}
\def\nodesize{4cm}

\newcommand{\numdims}[0]{2}
\newcommand{\numhidden}[0]{2}
\newcommand{\upnodedist}[0]{3cm}
\newcommand{\bardist}[0]{\hspace{-0.2cm}}

\newcommand{\neuronfunc}[2]{
\FPeval{\result}{clip(#1+#2)}
\includegraphics[width=5.5cm]{../../figures/two-d-draws/sqexp-draw-\result}}

\newlength{\arrowsize}  
\pgfarrowsdeclare{biggertip}{biggertip}{  
  \setlength{\arrowsize}{10pt}  
  \addtolength{\arrowsize}{2\pgflinewidth}  
  \pgfarrowsrightextend{0}  
  \pgfarrowsleftextend{-5\arrowsize}  
}{  
  \setlength{\arrowsize}{1pt}  
  \addtolength{\arrowsize}{\pgflinewidth}  
  \pgfpathmoveto{\pgfpoint{-5\arrowsize}{4\arrowsize}}  
  \pgfpathlineto{\pgfpointorigin}  
  \pgfpathlineto{\pgfpoint{-5\arrowsize}{-4\arrowsize}}  
  \pgfusepathqstroke  
} 


% Custom commmands.

\def\jointspacing{\vspace{0.3in}}

\def\boxwidth{0.21\columnwidth}
\newcommand{\gpdrawbox}[1]{
\setlength\fboxsep{0pt}
\hspace{-0.36in} 
\fbox{\hspace{-4mm}
\includegraphics[width=\boxwidth]{../figures/deep_draws/deep_gp_sample_layer_#1}
\hspace{-4mm}}}

\newcommand{\mappic}[1]{\hspace{-0.05in}\includegraphics[width=\boxwidth]{../../figures/seed-0-map/latent_coord_map_layer_#1}}
 
\newcommand{\mappiccon}[1]{\hspace{-0.05in}\includegraphics[width=\boxwidth]{../../figures/seed-0-map-connected/latent_coord_map_layer_#1}}

\newcommand{\spectrumpic}[1]{
\includegraphics[trim=5mm 0mm 4mm 3mm, clip, width=0.425\columnwidth]{../figures/spectrum/layer-#1}} 

\newcommand{\feat}{\vh}





\begin{document}
\begin{poster}
\vspace{1\baselineskip}   % Add some space at the top of the poster


%%% Header
\begin{center}
\begin{pcolumn}{1.03}

\newcommand{\logowidth}{0.09\textwidth}
\pbox{0.99\textwidth}{}{linewidth=2mm,framearc=0.3,linecolor=camdarkblue,fillstyle=gradient,gradangle=0,gradbegin=white,gradend=white,gradmidpoint=1.0,framesep=1em}{
%
%%% Cambridge Logo
\raisebox{-0cm}{
\begin{minipage}[c]{\logowidth}
  \begin{center}
    \includegraphics[width=6cm]{badges/University_Crest}
    \vspace{.1in}
    \includegraphics[width=6cm]{badges/unicamtext.pdf}
  \end{center}
\end{minipage}}
%
%%% Title
\begin{minipage}[c][9cm][c]{0.76\textwidth}
  \begin{center}
    {\sffamily \VeryHuge \textbf{Avoiding Pathologies in Very Deep Networks}}\\[10mm]
    {\huge\sffamily \Huge David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani\\[7.5mm]
     }
  \end{center}
\end{minipage}
%
% Harvard logo
\raisebox{-0cm}{
\begin{minipage}[c]{\logowidth}
  \begin{flushright}
    \includegraphics[width=8cm,trim=2em 0em 2em 2em, clip]{badges/harvard}
  \end{flushright}
\end{minipage}}
%
}
\end{pcolumn}
\end{center}

\vspace*{3cm}

\large


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Large

\begin{multicols}{2}


\mysection{Abstract}

\vspace*{-1.5cm}
\null\hspace*{3cm}\begin{minipage}[c]{0.8\columnwidth}
\centering
\begin{itemize}
\item We compare architectures by building priors over deep nets
\item Characterize a pathology in standard architecture
\item Show a simple alternative architecture which fixes the problem.
\end{itemize}
\end{minipage}

\jointspacing

\mysection{Nonparametric Priors on Deep Neural Networks}

\center
\begin{centering}
\input{tables/np-net.tex}
\end{centering}


Deep \gp{}s are compositions of functions, each $f^{(\ell)} \simind \GPt{0}{k(\vx, \vx')}$. 
\begin{align*}
\vf^{(1:L)}(\vx) = \vf^{(L)}(\vf^{(L-1)}(\dots \vf^{(2)}(\vf^{(1)}(\vx)) \dots))
\end{align*}

\vspace{0.5in} 
 
\mysection{Random Deep Nets Capture Few Degrees of Freedom}



A distribution warped by a function drawn from a deep \gp{} prior:
\vspace{0.5in}

\centering
\renewcommand{\tabcolsep}{0.5cm}
%\extracolsep{1cm}
\begin{tabular}{cccc}
Identity Map & 1 Layer & 4 Layers & 6 Layers \\
\gpdrawbox{1} & \gpdrawbox{2} & \gpdrawbox{4} & \gpdrawbox{6} \\
$p(\vx)$ & $p(\vf^{(1)}(\vx))$ & $p(\vf^{(1:4)}(\vx))$ &  $p(\vf^{(1:6)}(\vx))$
\end{tabular}

\jointspacing

As depth increases, the density concentrates along one-dimensional filaments.

\jointspacing\jointspacing\jointspacing


Sampled mappings illustrate properties of this prior on functions:
\jointspacing

\centering
\begin{tabular}{cccc}
Identity Map & 1 Layer & 2 Layers & 40 Layers \\
\hspace{-0.1in}
\includegraphics[width=\boxwidth]{../../figures/seed-0-map/layer_0} & \mappic{1} & \mappic{10} & \mappic{40} \\
$\vy = \vx$ & $\vy = \vf^{(1)}(\vx)$ & $\vy = \vf^{(1:2)}(\vx)$ & $\vy = \vf^{(1:40)}(\vx)$
\end{tabular}

\jointspacing

As depth increases, there is usually only one direction we can move $\vx$ to change $\vy$.







\newpage



\mysection{Good Representations Change Along All Tangents}
\jointspacing

\begin{tabular}{cc}
\begin{minipage}[c]{0.45\columnwidth}
\centering
\begin{tabular}{c}
Contours of representation \\
%\includegraphics[width=0.45\columnwidth]{figures/hidden_good} &
\begin{tikzpicture}[pile/.style={line width = 3pt, ->, >=stealth'}]
    \node[anchor=south west,inner sep=0] at (0,0) {
    	\includegraphics[clip, trim = 0cm 12cm 0cm 0.0cm, width=\columnwidth]{../figures/hidden_good}
    };
    \coordinate (D) at (2.7,2.35);
    \coordinate (Do) at (4.3,0.9);
    \coordinate (Dt) at (5,5);
    
    \draw[pile] (D) -- (Dt) node[above, text width=5em] { tangent };
    \draw[pile] (D) -- (Do) node[right, text width=5em] { orthogonal };
\end{tikzpicture}
\end{tabular}

\end{minipage}
&
\begin{minipage}[c]{0.5\columnwidth}
Representation $\vy = \vf(\vx)$ must change in directions tangent to the data manifold, to preserve information. { \color{mydarkblue} (Rifai et. al., 2011)}
\end{minipage}
\end{tabular}

\jointspacing






\mysection{Explaining the Pathology}




\begin{tabular}{cc}
\begin{minipage}[c]{0.4\columnwidth}

\begin{itemize}
\item Jacobian of a deep \gp{} is a product of independent Gaussian matrices.
\item Singular value spectrum shows relative size of derivatives.
\item As net deepens, one direction has much larger derivative than others.
\end{itemize}

\end{minipage}
&
\begin{minipage}[c]{0.55\columnwidth}
\begin{centering}
\begin{tabular}{cc}
2 layer spectrum & 6 layer spectrum \\
\hspace{-0.16in} \spectrumpic{2} &
\hspace{-0.16in} \spectrumpic{6} \\
Singular values & Singular values  
%2 layers & 4 layers & 6 layers
\end{tabular}
\end{centering}
\end{minipage}
\end{tabular}






\jointspacing
\jointspacing

\mysection{Fixing the pathology}
\centering
%\begin{tabular}{C{0.5\columnwidth}|c}
\begin{itemize}
	\item 
	Following {\color{mydarkblue} (Neal, 1995)}, we connect the input to every layer:
\end{itemize}

\jointspacing

\input{tables/input-connected}
%\end{tabular}
%Two different architectures for deep neural networks.  The standard architecture connects each layer's outputs to the next layer's inputs.  The input-connected architecture connects also connects the original input $\vx$ to each layer.

\jointspacing



\newcommand{\gpdrawboxcon}[1]{
\setlength\fboxsep{0pt}
\hspace{-0.4in} 
\fbox{
\includegraphics[width=0.47\columnwidth]{../../figures/connected_deep_sample_seed_0/deep_sample_connected_layer#1}
}}

\begin{tabular}{cc}
\begin{minipage}[c]{0.4\columnwidth}
Pathology is now resolved in deep density models:  Density does not concentrate along filaments when input connects to all layers.
\end{minipage}
&
\begin{minipage}[c]{0.45\columnwidth}
\centering
\begin{tabular}{cc}
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws/deep_gp_sample_layer_1} &
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws_connected/deep_sample_connected_layer2} &
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws_connected/deep_sample_connected_layer3} \\
%$p(\vx)$ & $p(f_1(\vx))$ & $p(f_2(f_1(\vx), \vx))$ \\ \\
%\gpdrawboxcon{2} &
 4 Layers & 5 Layers \\
\gpdrawboxcon{4} &
\gpdrawboxcon{5}
\end{tabular}
\end{minipage}
\end{tabular}

\jointspacing
\jointspacing

\vspace{0.3in}

\begin{tabular}{cccc}
Identity Map %$\vy = \vx$ 
& 2 Layers & 10 Layers & 40 Layers \\%\\2 Layers: $\vy = f_1(f_2(\vx))$ \\
\hspace{-0.5in} \hspace{-0.05in}\includegraphics[width=\boxwidth]{../../figures/seed-0-map-connected/layer_0} & \mappiccon{2} & \mappiccon{10} & \mappiccon{40}
%\mappic{4} & \mappic{10} & \mappic{40} \\
%4 Layers & 10 Layers & 
\end{tabular}

\jointspacing
Locally up to $D$ degrees of freedom, at any depth.





%\mysection{Conclusions}


%\raggedright

%\begin{tabular}{cc}
%\begin{minipage}[c]{0.8\columnwidth}

%\begin{itemize}
%	\item Random networks capture fewer degrees of freedom as they get deeper
%	\item Connecting the input to each layer resolves this pathology
%\end{itemize}


%\end{minipage}
%&
%\begin{minipage}[c]{0.2\columnwidth}
%\begin{centering}
%\includegraphics[width=\linewidth]{figures/qrcode-paper-link}
%\end{centering}
%\end{minipage}
%\end{tabular}
%
%
%

\end{multicols}


\vspace*{3cm}

\begin{center}
\begin{pcolumn}{1.03}

\pbox{0.99\textwidth}{}%{linewidth=2mm,framearc=0.3,linecolor=camdarkblue,fillstyle=gradient,gradangle=0,gradbegin=white,gradend=white,gradmidpoint=1.0,framesep=1em}
{}
{
%\begin{minipage}[c][9cm][c]{\textwidth}
  \begin{center}
    {\sffamily \VeryHuge \textbf{Other Analyses}}
  \end{center}
%\end{minipage}
}
\end{pcolumn}
\end{center}




\begin{multicols}{2}


\mysection{Dropout in Gaussian Processes}

\begin{tabular}{cc}
\begin{minipage}[c]{0.65\columnwidth}
\begin{itemize}
\item One-layer GPs are infinitely-wide neural nets
\item Dropping out features has no effect
\item Dropping out inputs gives mixture of GPs
\item This mixture has closed-form covariance $$\covarianceargs{}{f(\vx'), f(\vx)} = \frac{1}{2^D} \sum_{\vr \in \{0,1\}^D}  \prod_{d=1}^D k_d(\vx_d, \vx_d')^{r_d}$$
\end{itemize}
\end{minipage}
&
\begin{minipage}[c]{0.3\columnwidth}
\centering
\begin{tabular}{c}
\null\hspace*{-0.2in} \includegraphics[trim=0em 0em 0em 0em, clip, width=0.9\columnwidth]{figures/3d-kernel/3d_add_kernel_321}\\
Dropout covariance \\isocontour
\end{tabular}
\end{minipage}
\end{tabular}



\mysection{Infinitely Deep Kernels}
\vspace*{-1cm}
\begin{minipage}[c]{0.6\columnwidth}
\begin{itemize}
%\item Can also analyze fixed deep feature mappings:
%\item {\color{mydarkblue} (Cho, 2012) } built kernels from multiple layers of feature mappings:
\item Kernels correspond to feature mappings:
$${k_1(\vx, \vx') = \feat(\vx) \tra \feat(\vx')}$$
\item Compose feature maps for deep kernels:
% we can also build kernel 
$${k_2(\vx, \vx') = \feat(\feat(\vx)) \tra \feat(\feat(\vx'))}$$
%
%We can consider applying the feature transform $\Phi(\cdot)$ to the features themselves:  $\Phi_2 = \Phi(\Phi(\vx))$.  

%\item Recurrent limit for squared-exp kernel: % for any set of starting features $\Phi_n(\vx)$:
%
%In this section, we take the infinite limits of these compositions, and propose a new variant.
%
%One can derive a Gaussian process as a neural network: $f(x) = {\mathbf \alpha}^T \Phi(x) = \sum_{i=i}^K \alpha_i \phi_i(x)$.  
%
%
%we derive a kernel which corresponds to arbitrarily many compositions of the feature vectors corresponding to the squared-exp kernel:
%
%\begin{align*}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
%& k_{n+1}(\vx, \vx') = \nonumber \\
%& = \exp \left( -\frac{1}{2} \left|\left| \left[ \! \begin{array}{c} \Phi_n(\vx) \\ \vx \end{array} \! \right]  - \left[ \! \begin{array}{c} \Phi_n(\vx') \\ \vx' \end{array} \! \right] \right| \right|_2^2 \right) \nonumber \\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( -\frac{1}{2} \sum_i \left[ \phi_i(\vx) - \phi_i(\vx') \right]^2 -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp\left ( -\frac{1}{2} \sum_i \left[ \phi_i(\vx)^2 - 2 \phi_i(\vx) \phi_i(\vx') + \phi_i(\vx')^2 \right]  -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \phi_i(\vx)^2 - 2 \sum_i \phi_i(\vx) \phi_i(\vx') + \sum_i \phi_i(\vx')^2 \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ k_1(\vx, \vx) - 2 k_1(\vx, \vx') + k_1(\vx', \vx') \right] \right) \\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( k_n(\vx, \vx') - 1 -\frac{1}{2} || \vx - \vx' ||_2^2 \right)
%\end{align*}

%\item %This kernel satisfies 

%\item No closed form, but continuous and differentiable everywhere except at $\vx = \vx'$.
\end{itemize}
\end{minipage}
\begin{minipage}[c]{0.39\columnwidth}
\begin{centering}
\begin{tabular}{c}
%\hspace{-0.5cm}\includegraphics[width=0.33\columnwidth, clip, trim = 0cm 0cm 1cm 0.61cm]{../figures/deep_kernel} &
Deep connected kernel \\
\hspace{-0.5cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0.4cm 0.9cm 0.3cm]{../figures/deep_kernel_connected}
 \\
%Connected \gp{} draws \\
%\hspace{-0.5cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0.1cm 0.9cm 0.35cm]{../figures/deep_kernel_connected_draws}
$k_\infty = \log(k_\infty) + 1 + \frac{1}{2} || \vx - \vx' ||_2^2$
\end{tabular}
\end{centering}
\end{minipage}

Code at {\color{mydarkblue}{\texttt{github.com/duvenaud/deep-limits}}}\\
Paper at {\color{mydarkblue}{\texttt{arxiv.org/abs/1402.5836}}}

\end{multicols}
\end{poster}
\end{document}
