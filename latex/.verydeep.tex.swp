\documentclass[twoside]{article}
\usepackage[accepted]{include/aistats2014}
\usepackage{times}
\usepackage{include/preamble}
\usepackage{natbib}
\usepackage{tikz}
\usetikzlibrary{arrows}



%% For submission, make all render blank.
\input{include/commenting.tex}
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!


\newcommand{\numdims}[0]{3}
\newcommand{\numhidden}[0]{4}
\newcommand{\upnodedist}[0]{0.6cm}
\newcommand{\bardist}[0]{\hspace{-0.2cm}}

% HUMBLE WORDS: shown slightly smaller when in normal text
% Thanks to Christian Steinruecken!
\makeatletter%
\newlength{\nonHumbleHeight}
\def\@humbleformat#1{{\settoheight{\nonHumbleHeight}{#1}\resizebox{!}{0.94\nonHumbleHeight}{#1}}}%
\newcommand\humble[1]{{\@humbleformat{#1}}}%
\makeatother%


\newcommand{\gp}{{\humble{GP}}}
\newcommand{\gpt}{{\sc gp}}
\newcommand{\MLP}{{\humble{MLP}}}

% Unify notation between neural-net land and GP-land.
\newcommand{\hphi}{h}
\newcommand{\hPhi}{\vh}
\newcommand{\walpha}{w}
\newcommand{\wboldalpha}{\bw}
\newcommand{\wcapalpha}{\vW}

\newcommand{\layerindex}{\ell}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\sectiondist}{}

\begin{document}

\twocolumn[
\aistatstitle{Avoiding pathologies in very deep neural networks}
\aistatsauthor{ David Duvenaud \And Oren Rippel \And Ryan Adams \And Zoubin Ghahramani }
\aistatsaddress{ 
University of Cambridge \And %\\ \texttt{ \small  dkd23@cam.ac.uk} \And 
M.I.T., Harvard University \And %\\ \texttt{ \small rippel@math.mit.edu} \And 
Harvard University \And %\\ \texttt{ \small  rpa@seas.harvard.edu} \And 
University of Cambridge} %\texttt{ \small zoubin@eng.cam.ac.uk}}
]


\begin{abstract}
Choosing appropriate architectures and regularization strategies of deep networks is crucial to good predictive performance.  To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions.  Specifically, we study deep Gaussian processes, a type of infinitely-wide, deep neural network.  We show that in these architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit.  We propose alternate network architectures which do not suffer from these pathologies.  We also derive deep covariance functions, obtained by composing infinitely many feature transforms.  Lastly, we characterize the models obtained by performing dropout on Gaussian processes.
\end{abstract}


\section{Introduction}
\sectiondist

Much recent work on deep networks has focused on weight initialization \citep{martens2010deep}, regularization \citep{lee2007sparse} and network architecture \citep{gens2013learning}.
However, the interactions between these different design decisions can be complex and difficult to characterize.
We propose to approach the design of deep architectures by examining the problem of assigning priors to deep functions.
% Although inference in fully Bayesian models is generally challenging, well-defined priors allow us to encode our beliefs into models in a data-independent way. 
Well-defined priors allow us to explicitly examine the assumptions being made about functions we may wish to learn.
Once we identify classes of priors that imbue our models with desirable properties, these in turn may suggest regularization, initialization, and architecture choices that delineate such properties.

Fundamentally, a multilayer neural network implements a composition of vector-valued functions, one per layer.
Hence, understanding properties of such function compositions helps us gain insight into deep networks.
In this paper, we examine a simple and flexible class of priors on compositions of functions, namely deep Gaussian processes \citep{damianou2012deep}.
Deep \gp{}s are simply priors on compositions of vector-valued functions, where each output of each layer is drawn independently from a GP prior:
%
\begin{align}
\vf^{(1:L)}(\vx) & = \vf^{(L)}(\vf^{(L-1)}(\dots \vf^{(2)}(\vf^{(1)}(\vx)) \dots)) \\
\vf_d^{(\layerindex)} & \simind \GPt{0}{k^\layerindex_d(\vx, \vx')}
\end{align}
%
%Although inference in these models is non-trivial,
These models correspond to a certain type of infinitely-wide multi-layer perceptron (\MLP{}), and as such make canonical candidates for generative models of functions that closely relate to neural networks.

By characterizing these models, this paper shows that representations based on repeated composition of independently-initialized functions exhibit a pathology where the representation becomes invariant to all but one direction of variation. This corresponds to an eventual debilitating decrease in the information capacity of networks as a function of their number of layers. However, we will demonstrate that a simple change in architecture --- namely, connecting the input to each layer --- fixes this problem. 

We also present two related analyses:  First, we examine the properties of a arbitrarily deep fixed feature transforms (``deep kernels'').  Second, we characterise the prior obtained by performing dropout on \gp{}s, showing equivalences to existing models.

%Section \ref{sec:relating} will derive the precise relationship between deep \gp{}s and MLPs.  Section \ref{sec:1d} will examine the asymptotic behavior of arbitrarily deep one-dimensional \gp{}s.  Section \ref{sec:theorem} gives a theorem characterizing the Jacobian of a deep multidimensinal \gp{}.  Section 
%Deep networks have become an important tool for machine learning [cite].  However, training these models are difficult, Many arguments have been made for the need for deep architectures [cite Bengio].  However, it is hard to know what effect the deepness of an architecture has.  Also, the weights don't necessarily move that much from their initialization.

%Good initialization schemes can help, but we are interested in creating distributions on deep functions so that most of the mass is on functions without any pathologies.  Also, we might be able to show that the pathologies we show that most functions exhibit means that it will be hard to learn them by gradient descent.







\section{Relating deep neural nets and deep Gaussian processes}
%\section{RELATING DEEP NETS AND DEEP GAUSSIAN PROCESSES}
\label{sec:relating}
\sectiondist
%There are two ways to view deep Gaussian processes as neural networks.  First, we can
%We introduce a generative non-parametric model to address this problem.  Our approach is based on the GP-LVM ~\cite{lawrence2004gaussian,salzmann2008local,lawrence2009non}, a flexible nonparametric density model.  

%\cite{NIPS2005_424} showed that kernel machines such as \gp{}s have limited generalization ability when they are forced to use a 'local' kernel.

%\subsection{A neural net with one hidden layer}
%\vspace{-0.05in}
\subsection{Neural nets with one hidden layer}

\begin{figure*}
\input{tables/architecture-comparison-aistats}
\caption{Comparing architectures.
In the deep \gpt{} models, there are two possible meanings for the hidden units.  
We can consider every other layer to be a linear combination of an infinite number of parametric hidden units. Alternatively, we can integrate out the hidden layers, and consider the deep \gpt{} to be a neural network with a finite number of hidden units, each with a different non-parametric activation function.}
\label{fig:architectures}
\end{figure*}

In the typical definition of an \MLP{}, the hidden units of the first layer are defined as:
%
\begin{align}
\vh^{(1)}(\vx) = \sigma \left( \vb^{(1)} + \vW^{(1)}\vx \right)
\end{align}
%
where $\vh$ are the hidden unit activations, $\vb$ is a bias vector, $\vW$ is a weight matrix and $\sigma$ is a one-dimensional nonlinear function applied element-wise. The output vector $f(\vx)$ is simply a weighted sum of these hidden unit activations:
%
\begin{align}
f(\vx) = \vV^{(1)} \sigma \left( \vb^{(1)} + \vW^{(1)} \vx \right)  = \vV^{(1)} \vh^{(1)}(\vx) 
\end{align}
%
where $\vV^{(1)}$ is another weight matrix.

There exists a correspondence between one-layer \MLP{}s and \gp{}s \citep{neal1995bayesian}.  \gp{} priors can be viewed as a prior on neural networks with infinitely many hidden units.  More precisely, for any model of the form
%
\begin{align}
f(\vx) = \frac{1}{K}{\mathbf \alpha}\tra \hPhi(\vx) = \frac{1}{K} \sum_{i=1}^K \alpha_i \hphi_i(\vx),
\label{eq:one-layer-nn}
\end{align}
%
with fixed features $\left[ \hphi_1(\vx), \dots, \hphi_K(\vx) \right]\tra = \hPhi(\vx)$ and i.i.d. $\alpha$'s with zero mean and finite variance $\sigma^2$, the central limit theorem implies that as the number of features $K \rightarrow \infty$, any two function values $f(\vx), f(\vx')$ have a joint distribution approaching $\Nt{0}{\frac{\sigma^2}{K}\sum_{i=1}^K \hphi_i(\vx)\hphi_i(\vx')}$.
A joint Gaussian distribution between any two function values is the definition of a Gaussian process.
\TBD{DD: any two or any set?}

The result is surprisingly general:
It doesn't put any constraints on what the features are (other than having bounded activation), nor does it require that the feature weights $\alpha$ be Gaussian distributed.  

We can also work backwards to derive a one-layer \MLP{} from a \gp{}:  Mercer's theorem implies that any positive-definite kernel function corresponds to an inner product of features: $k(\vx, \vx') = \hPhi(\vx) \tra \hPhi(\vx')$.
%
Thus in the one-hidden-layer case, the correspondence between \MLP{}s and \gp{}s is simple:  The features $\hphi(\vx)$ of the \gp{} correspond to the hidden units of the \MLP{}.


\subsection{Multiple hidden layers}

%\paragraph{Fixed nonlinearities}
In an \MLP{}, the $\ell^{th}$ layer units are given by the recurrence:
%
\begin{align}
\vh^{(\layerindex)}(\vx) = \sigma \left( \vb^{(\layerindex)} + \vW^{(\layerindex)} \vh^{(\layerindex-1)}(\vx) \right)
\end{align}
This architecture is shown in figure \ref{fig:architectures}b. 
In this model, each hidden layer's output feeds directly into the next layer's input, weighted by the corresponding element of $\vW^{(\layerindex)}$.  

If we extend the model given by \eqref{eq:one-layer-nn} to have multiple layers of fixed feature mappings, but unknown weights on the final layer, the resulting model
%
\begin{align}
f(\vx) = \frac{1}{K}{\mathbf \alpha}\tra \hPhi^{(2)}\left( \hPhi^{(1)}(\vx) \right)
% = \frac{1}{K} \sum_{i=1}^K \alpha_i \hphi_i(\vx),
\label{eq:mutli-layer-nn}
\end{align}
corresponds a \gp{} with a ``deep kernel'': 
\begin{align}
k(\vx, \vx') = \hPhi^{(2)} \left( \hPhi^{(1)}(\vx)) \tra \hPhi^{(2)}(\hPhi^{(1)}(\vx') \right)
\end{align}
These models will be considered in section \ref{sec:deep_kernels}.  However, these models imply a fixed representation, as opposed to a prior over reprentations, which is what we wish to analyze in this paper.


%In a neural net with multiple hidden layers, the correspondence is a little more complicated.
 

We wish to construct a neural network with fixed nonlinearities corresponding to a deep \gp{}, with $D_\layerindex$ units for each layer corresponding to the outputs compositions of functions drawn from a \gp{} prior.  To do so, we must introduce a second layer in between each infinitely-wide set of fixed basis functions.
The $D_\layerindex$ outputs $\vf^{(\layerindex)}(\vx)$ in between each layer are weighted sums (with unknown weights) of the hidden units of the layer below, and the next layer's hidden units depend only on these $D$ outputs.
Thus, the network architecture with fixed nonlinearities corresponding to deep \gp{}s have an extra set of layers that a \MLP{} doesn't have, shown in figure \ref{fig:architectures}c.

Since the hidden units in a deep \gp{} implied by Mercer's theorem $\hphi^{(\layerindex)}(\vx)$ depend only on a linear projection of their inputs $\hphi(\vx) = \sigma \left( \vb^{(\layerindex)} + \vW^{(\layerindex)}f^{(\layerindex-1)}(\vx) \right)$, we can simply substitute $\vf^{(\layerindex-1)}(\vx) = \vV^{(\layerindex-1)} \hphi^{(\layerindex-1)}(\vx)$ to recover $\hphi(\vx) = \sigma \left( \vb^{(\layerindex)} + \vW^{(\layerindex)} \vV^{(\layerindex-1)} \hphi^{(\layerindex-1)}(\vx) \right)$.
Thus, we can ignore the intermediate outputs $\vf^{(n)}(\vx)$, and exactly recover an MLP with activation functions given by Mercer's theorem, but with unkown rank-$D$ weight matrices 
%$W^{(n)} V^{(n-1)}$
 between layers.



%There are two ways to relate deep \gp{}s to \MLP{}s.
A more direct way to construct a network architecture corresponding to a deep \gp{} is to integrate out all $\vV^{(n)}$, and view deep \gp{}s as a neural network with a finite number of nonparametric, \gp{}-distributed basis functions at each layer, where the $D$ outputs of $\vf^{1:\layerindex}(\vx)$ represent the output of the hidden nodes at the $\layerindex^{th}$ layer.
This second view lets us compare deep \gp{} models to multilayer perceptrons more directly, examining the activations and shapes of the finite number of basis functions at each layer.


%Figure \ref{fig:architectures} compares these two architectures.  If we stick to the correspondence between neural nets and \gp{}s as defined in \eqref{eq:one-layer-nn}, we notice that the deep \gp{} architecture forces the linear connections between hidden layers go be squeezed through a finite set of nodes, corresponding to the output of the independent \gp{}s at each layer.  




%\subsection{Relating initialization strategies, regularization, and priors}

%Traditional training of deep networks requires both an an initialization strategy, and a regularization method.  In a Bayesian model, the prior implicitly defines both of these things.

%For example, one feature of \gp{} models with a clear analogue to initialization strategies in deep networks is automatic relevance determination. 
%\paragraph{The effects of automatic relevance determination}  One feature of Gaussian process models that doesn't have a clear anologue in the neural-net literature is automatic relevance determination (ARD).  Recall that 
%In the ARD procedure, the lengthscales of each dimension are scaled to maximize the marginal likelihood of the GP.  In standard one-layer GP regression, it has the effect of smoothing out the posterior over functions in directions in which the data does not change.

%\paragraph{Sparse Initialization}
%\cite{martens2010deep} used “sparse initialization”, in which each hidden unit had 15 non-zero incoming connections.  %They say ``this allows the units to be both highly differentiated as well as unsaturated, avoiding the problem in dense initializations where the connection weights must all be scaled very small in order to prevent saturation, leading to poor differentiation between units''.
%
%While it is not clear if deep GPs have analogous problems with saturation of connection weights, we note that 
%An anologous initialization strategy would be to put a heavy-tailed prior on the lengthscales in the squared-exp kernel.


%\subsection{What kinds of functions do deep GPs prefer?}
%Isotropic kernels such as the squared-exp encode the assumption that the function varies independently in all directions.  
%This implies that using an isotropic kernel to model a function that does \emph{not} vary in all direction independently is 'wasting marginal likelihood' by not making strong enough assumptions.  
%
%Thus, deep GPs prefer to have hidden layers on whose outputs the higher-layer functions vary independently.  In this way, we can say that the deep GP model prefers independent features.

%Combining these two properties of isotropic kernels, we can say that a deep GP will attempt to transform its input into a representation with as few degrees of freedom as possible, and for which the output function varies independently across each dimension.





\section{One-dimensional asymptotics}
\sectiondist
\label{sec:1d}

One way to understand properties of functions drawn from deep GPs and deep networks by looking at the distribution of the derivative of these functions. We first focus on the one-dimensional case.
%
In this section, we derive the limiting distribution of the derivative of an arbitrarily deep, one-dimensional \gp{} with a squared-exp kernel:  %in order to shed light on the ways in which a deep \gp{}s are non-Gaussian, and the ways in which lower layers affect the computation performed by higher layers.
%
\newcommand{\onedsamplepic}[1]{
\hspace{-0.2in}
\includegraphics[trim=2mm 2mm 2mm 6.4mm, clip, width=0.5\columnwidth]{figures/1d_samples/latent_seed_0_1d_large/layer-#1}} 
\newcommand{\onedsamplepiccon}[1]{
\hspace{-0.2in}
\includegraphics[trim=2mm 2mm 2mm 6.4mm, clip, width=0.5\columnwidth]{figures/1d_samples/latent_seed_0_1d_large_connected/layer-#1}} 
\begin{figure}
\centering
\begin{tabular}{cc}
\hspace{-0.1in} One layer & \hspace{-0.2in} 2-Layer Composition \\
\hspace{0.03in}
\onedsamplepic{1} &
\onedsamplepic{2} \\
\hspace{-0.2in}  5-Layer Composition & \hspace{-0.25in} 10-Layer Composition \\
\onedsamplepic{5} &
\onedsamplepic{10}
%& 6 layers \\ & largest , for unconnected layers & largest, for connected layers
\end{tabular}
\caption{One-dimensional draws from a deep \gpt{} prior.  After a few layers, the functions begin to be either nearly flat, or highly varying, everywhere.  This is a consequence of the distribution on derivatives becoming heavy-tailed.}
\label{fig:deep_draw_1d}
\end{figure}
%
%
%First, we derive the limiting distribution for the derivative of a one-dimensional deep \gp{}.
%
%
\begin{align}
k_{\textnormal{SE}}(x, x') = \sigma^2_f \exp \left( \frac{-(x - x')^2}{2\ell^2} \right)
\end{align}
%
The hyperparameter $\sigma^2_f$ controls the variance of functions drawn from the prior, and the hyperparameter $\ell$ controls the smoothness.  
The derivative of a \gp{} with a squared-exp kernel is distributed as $\Nt{0}{\nicefrac{\sigma^2_f}{\ell^2}}$.  
Intuitively, a \gp{} is likely to have large derivatives if it has high variance and small lengthscales.
 
 By the chain rule, the derivative of a one-dimensional deep \gp{} is simply a product of its (independent) derivatives.  
 The distribution of the absolute value of this derivative is a product of half-normals, each with mean $\sqrt{\nicefrac{2 \sigma^2_f}{\pi\ell^2}}$.
%
%\begin{align}
%\frac{\partial f(x)}{\partial x} & \distas{iid} \Nt{0}{\frac{\sigma^2_f}{\ell^2}} \\
%\implies 
%\left| \frac{\partial f(x)}{\partial x} \right| & \distas{iid} \textnormal{half}\Nt{\sqrt{\frac{2 \sigma^2_f}{\pi\ell^2}}}{\frac{\sigma^2_f}{\ell^2} \left( 1 - \frac{2}{\pi} \right)}
%\end{align}

Thus, if we choose kernel parameters such that $\nicefrac{\sigma^2_f}{\ell_{d_1}^2} = \nicefrac{\pi}{2}$, then $\expectargs{}{\left| \nicefrac{\partial f(x)}{\partial x} \right|} = 1$, and so $\expectargs{}{\left| \nicefrac{\partial f^{(1:L)}(x)}{\partial x} \right|} = 1$, that is to say, the expected magnitude of the derivative remains constants no matter the depth.  If $\nicefrac{\sigma^2_f}{\ell^2}$ is less than $\nicefrac{\pi}{2}$, the expected derivative magnitude goes to zero, and if greater, the expected magnitude goes to infinity as a function of $L$.  

The log of the magnitude of the derivatives has moments:
\begin{align}
m_{\log} = \expectargs{}{\log \left| \frac{\partial f(x)}{\partial x} \right|} & = 2 \log \left( \frac{\sigma_f}{\ell} \right) - \log2 - \gamma \\
%\varianceargs{}{\log \left| \frac{\partial f(x)}{\partial x} \right|} & = \frac{1}{4}\left[ \pi^2 + 2 \log^2 2  - 2 \gamma( \gamma + log4 ) + 8 \left( \gamma + \log2 - \log \left(\frac{\sigma_f}{\ell}\right)\right) \log\left(\frac{\sigma_f}{\ell}\right) \right]
v_{\log} = \varianceargs{}{\log \left| \frac{\partial f(x)}{\partial x} \right|} & = \frac{\pi^2}{4} + \frac{\log^2 2}{2}  - \gamma^2 - \gamma \log4 \nonumber \\
& \hspace{-2cm} + 2 \log\left(\frac{\sigma_f}{\ell}\right) \left[ \gamma + \log2 - \log \left(\frac{\sigma_f}{\ell}\right) \right]
\end{align}
where $\gamma \approxeq 0.5772$ is Euler's constant.  Since the second moment is finite, by the central limit theorem, the limiting distribution of the size of the gradient approaches log-normal as L grows:
\begin{align}
\log \left| \frac{\partial f^{1:L}(x)}{\partial x} \right| 
& = \sum_{i=1}^L \log \left| \frac{\partial f^i(x)}{\partial x} \right| \nonumber \\
 \implies
% \distas{L \rightarrow \infty} \Nt{L ?}{L ?}
%\log \left| \frac{\partial f^{1:L}(x)}{\partial x} \right| & \distas{L \rightarrow \infty} \Nt{L \sqrt{\frac{2 \sigma^2_f}{\pi\ell^2}}}{L \frac{\sigma^2_f}{\ell^2} \left( 1 - \frac{2}{\pi} \right)}
\log \left| \frac{\partial f^{1:L}(x)}{\partial x} \right| & \distas{L \rightarrow \infty} \Nt{ L m_{\log} }{L^2 v_{\log}}
\end{align}
%
Even if the expected magnitude of the derivative remains constant, the variance of the log-normal distribution grows without bound as the depth increases.  Because the log-normal distribution is heavy-tailed, and its domain is bonded below by zero, the derivative will become very small almost everywhere, with rare but very large jumps.  

Figure \ref{fig:deep_draw_1d} shows this behavior in a draw from a 1D deep \gp{} prior, at varying depths.  This figure also shows that once the derivative in one region of the input space becomes very large or very small, it is likely to remain that way in subsequent layers.
%
%Does it convege to some sort of jump process?  
%Note that there is another limit - if $\nicefrac{\sigma^2_f}{\ell_{d_1}^2} < \nicefrac{\pi}{2}$, then the mean of the size of the gradient goes to zero, but the variance approaches a finite constant.







\section{The Jacobian of deep GP is a product of independent normal matrices}
\sectiondist
\label{sec:theorem}

We now derive the distribution on Jacobians of multivariate functions drawn from a deep \gp{} prior.

\begin{lemma}
\label{thm:deriv-ind}
The partial derivatives of a function mapping $\mathbb{R}^D \rightarrow \mathbb{R}$ drawn from a \gp{} prior with a product kernel are independently Gaussian distributed.
\end{lemma}
%
%\vspace{-0.2in}
\begin{proof}
Because differentiation is a linear operator, the derivatives of a function drawn from a \gp{} prior are also jointly Gaussian distributed.  The covariance between partial derivatives w.r.t. input dimensions $d_1$ and $d_2$ of vector $\vx$ are given by \citet{Solak03derivativeobservations}:
%
\begin{align}
\cov \left( \frac{\partial f(\vx)}{\partial x_{d_1}}, \frac{\partial f(\vx)}{\partial x_{d_2}} \right) 
= \frac{\partial^2 k(\vx, \vx')}{\partial x_{d_1} \partial x_{d_2}'} \bigg|_{\vx=\vx'}
\end{align}
%
If our kernel is a product over individual dimensions $k(\vx, \vx') = \prod_d^D k_d(x_d, x_d')$, as in the case of the squared-exp kernel, then the off-diagonal entries are zero, implying that all elements are independent.
\end{proof}

%\vspace{-0.1in}
In the case of the multivariate squared-exp kernel, the covariance between derivatives has the form:
%
\begin{align}
%k_{SE}(\vx, \vx') = 
f(\vx) \sim \textnormal{GP}\left( 0, 
\sigma^2_f \prod_{d=1}^D \exp \left(-\frac{1}{2} \frac{(x_d - x_d')^2}{\ell_d^2} \right) \right) \nonumber \\
 \implies 
\cov \left( \frac{\partial f(\vx)}{\partial x_{d_1}}, \frac{\partial f(\vx)}{\partial x_{d_2}} \right) =
%\frac{\partial^2 k_{SE}(\vx, \vx')}{\partial x_{d_1} \partial x_{d_2}'} \bigg|_{\vx=\vx'} =
\begin{cases} 
\frac{\sigma^2_f}{\ell_{d_1}^2} & \mbox{if } d_1 = d_2 \\ 
0 & \mbox{if } d_1 \neq d_2 \end{cases}
\end{align}


\begin{lemma}
\label{thm:matrix}
The Jacobian of a set of $D$ functions $\mathbb{R}^D \rightarrow \mathbb{R}$ drawn independetly from a \gp{} prior with a product kernel is a $D \times D$ matrix of independent Gaussian R.V.'s
\end{lemma}
%
%\vspace{-0.2in}
\begin{proof}
The Jacobian of the vector-valued function $\vf(\vx)$ is a matrix $J$ with elements ${J_{ij} = \frac{ \partial \vf_i (\vx) }{\partial x_j}}$.
%
%\begin{align}
%J_{ij} = \dfrac{\partial f_i (\vx) }{\partial x_j}
%\end{align}
%\begin{align}
%\Jx^\ell(\vx) =\begin{bmatrix} \dfrac{\partial f^\ell_1 (\vx) }{\partial x_1} & \cdots & \dfrac{\partial f^\ell_1 (\vx)}{\partial x_D} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial f^\ell_D (\vx)}{\partial x_1} & \cdots & \dfrac{\partial f^\ell_D (\vx)}{\partial x_D}  \end{bmatrix}
%\end{align}
%
%
Because we've assumed that the \gp{}s on each output dimension $f_d(\vx) \sim \GP$ are independent, it follows that each row of $J$ is independent.
Lemma \ref{thm:deriv-ind} shows that the elements of each row are independent Gaussian.
Thus all entries in the Jacobian of a \gp{}-distributed transform are independent Gaussian R.V.s.
\end{proof}

\begin{theorem}
\label{thm:prodjacob}
The Jacobian of a deep \gp{} with a product kernel is a product of independent Gaussian matrices, with each entry in each matrix being drawn independently.
\end{theorem}
%
%\vspace{-0.2in}
\begin{proof}
When composing $L$ different functions, we'll denote the \emph{immediate} Jacobian of the function mapping from layer $\ell -1$ to layer $\ell$ as $J^\ell(\vx)$, and the Jacobian of the entire composition of $L$ functions by $J^{1:L}(\vx)$.

By the multivariate chain rule, the Jacobian of a composition of functions is simply the product of the Jacobian matrices of each function.  
%
Thus the Jacobian of the composed (deep) function $\vf^{(L)}(\vf^{(L-1)}(\dots \vf^{(3)}( \vf^{(2)}( \vf^{(1)}(\vx)))\dots))$ is
%
\begin{align}
 J^{1:L}(x) 
% = \frac{\partial \fdeep(x) }{\partial x} 
%= \frac{\partial f^1(x) }{\partial x} \frac{\partial f^{(2)}(x) }{\partial f^{(1)}(x)} \cdots \frac{\partial f^L(x) }{\partial f^({L-1})(x)}
%= \prod_{\ell = 1}^{L} J^L (x)
= J^L J^{L-1} \dots J^3 J^2 J^1. 
\end{align}
%
By lemma \ref{thm:matrix}, each ${J^\ell_{i,j} \simind \Nt{0}{\frac{\sigma^2_f}{\ell^2}}}$, so the complete Jacobian is a product of independent Gaussian matrices, and each entry of those matrices is drawn independently.
\end{proof}

\vspace{-0.1in}
Theorem \ref{thm:prodjacob} allows us to analyze the representational properties of a deep Gaussian process by simply examining the properties of products of independent Gaussian matrices, a well-studied object.






\section{Formalizing a pathology}
\sectiondist

%\subsection{Jacobian Singular Value Spectrum}

\cite{rifai2011higher} argue that a good latent representation is invariant in directions orthogonal to the manifold on which the data lie.  Conversely, a good latent representation must also change in directions tangent to the data manifold, in order to preserve relevant information.  Figure \ref{fig:hidden} visualizes this idea.
%
\begin{figure}[h!]
\centering
\begin{tabular}{c}
%\includegraphics[width=0.45\columnwidth]{figures/hidden_good} &
\begin{tikzpicture}[pile/.style={thick, ->, >=stealth'}]
    \node[anchor=south west,inner sep=0] at (0,0) {
    	\includegraphics[clip, trim = 0cm 12cm 0cm 0.0cm, width=0.9\columnwidth]{figures/hidden_good}
    };
    \coordinate (D) at (1.6,1.5);
    \coordinate (Do) at (2.1,0.9);
    \coordinate (Dt) at (2.8,2.4);
    
    \draw[pile] (D) -- (Dt) node[right, text width=5em] { tangent };
    \draw[pile] (D) -- (Do) node[right, text width=5em] { orthogonal };
\end{tikzpicture} \\
a) A noise-tolerant representation (blue \& green) \\ of a one-dimensional manifold (white)
%\includegraphics[clip, trim = 0cm 12cm 0cm 0.0cm, width=0.9\columnwidth]{figures/hidden_bad} \\
%b) A na\"{i}ve representation (colors) \\ of a one-dimensional manifold (white)
\end{tabular}
\caption{Representing a 1-D manifold.
%A representation is a function mapping the input space to some set of outputs.
Colors show the output of the computed representation as a function of the input space.
The representation is invariant in directions orthogonal to the data manifold, making it robust to noise in those directions, and reducing the number of parameters needed to represent a datapoint.  It also changes in directions tangent to the manifold, preserving information for later layers. 
%Representation b) changes in all directions, preserving potentially useless information.}% The representation on the right might be useful if the data were spread out in over plane.
}
\label{fig:hidden}
\end{figure}
%
We follow \cite{rifai2011contractive} in characterizing the representational properties of a function by the singular value spectrum of the Jacobian\footnote{ \cite{rifai2011contractive} examine the Jacobian at the training points, but the models we are examining are stationary, so it doesn't matter where we examine the function.}.  
%
\newcommand{\spectrumpic}[1]{
%\hspace{-0.2in}
\includegraphics[trim=5mm 0mm 4mm 6.4mm, clip, width=0.5\columnwidth]{figures/spectrum/layer-#1}} 
\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
2 layers & 
%4 layers & 
6 layers \\
\hspace{-0.16in} \spectrumpic{2} &
%\spectrumpic{4} &
\hspace{-0.16in} \spectrumpic{6} 
%2 layers & 4 layers & 6 layers
\end{tabular}
\caption{Normalized singular value spectrum of the Jacobian of a deep GP.  As the net gets deeper, the largest singular value dominates.  This implies that with high probability, there is only one effective degree of freedom in the representation being computed.  As depth increases, the distribution on singular values also becomes heavy-tailed.}
\label{fig:deep_spectrum}
\end{figure}
%
Figure \ref{fig:deep_spectrum} shows the spectrum for 5-dimensional deep GPs of different depths.  As the net gets deeper, the largest singular value dominates, implying there is usually only one effective degree of freedom in representation being computed.
%
%
\newcommand{\gpdrawbox}[1]{
\setlength\fboxsep{0pt}
\hspace{-0.15in} 
\fbox{
\includegraphics[width=0.464\columnwidth]{figures/deep_draws/deep_gp_sample_layer_#1}
}}
\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\gpdrawbox{1} &
\gpdrawbox{2} \\
$p(\vx)$ & $p(\vf^{(1)}(\vx))$ \\
\gpdrawbox{4} & 
\gpdrawbox{6} \\
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws/deep_gp_sample_layer_3} \\
%$p(\vx)$ & $p(f_1(\vx))$ & $p(f_2(f_1(\vx)))$ \\ \\
%\fbox{\includegraphics[width=0.24\columnwidth]{figures/deep_draws/deep_gp_sample_layer_4}} &
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws/deep_gp_sample_layer_5} &
%\fbox{\includegraphics[width=0.24\columnwidth]{figures/deep_draws/deep_gp_sample_layer_6}} \\
%$p(f_3(f_2(f_1(\vx))))$ & $p(f_4(f_3(f_2(f_1(\vx)))))$ & $p(f_5(f_4(f_3(f_2(f_1(\vx)))))$
$p(\vf^{(1:4)}(\vx))$ &  $p(\vf^{(1:6)}(\vx))$
%density of $\vf^{(1:4)}(\vX))$ & density of $\vf^{(1:6)}(\vX))$
\end{tabular}
\caption{Visualizing draws from a deep \gpt{}.  A 2-dimensional Gaussian distribution (top left) is warped by successive functions drawn from a \gpt{} prior.  As the number of layers increases, the density concentrates along one-dimensional filaments.}
\label{fig:filamentation}
\end{figure}
%
Figure \ref{fig:filamentation} demonstrates a related pathology that arises when composing functions to produce a deep density model.  The density in observed space eventually becomes locally concentrated onto one-dimensional manifolds, or \emph{filaments}, implying that such models are insuitable to model manifolds of greater than one dimension.

\newcommand{\mappic}[1]{\hspace{-0.05in}\includegraphics[width=0.46\columnwidth]{figures/map/latent_coord_map_layer_#1}} 
\newcommand{\mappiccon}[1]{\hspace{-0.05in} \includegraphics[width=0.46\columnwidth]{figures/map_connected/latent_coord_map_layer_#1}}
\begin{figure}[h!]
\centering
\begin{tabular}{cc}
Identity Map: $\vy = \vx$ & 1 Layer: $\vy = f^1(\vx)$ \\
\mappic{0} & \mappic{1} \\
 2 Layers: $\vy = f^{1:2}(\vx)$ & 40 Layers \\%\\2 Layers: $\vy = f_1(f_2(\vx))$ \\
\mappic{2} & 
\begin{tikzpicture}[pile/.style={thick, ->, >=stealth'}]
    \node[anchor=south west,inner sep=0] at (0,0) {
    	\mappic{40}
    };
    \coordinate (D) at (0.3,1.4);
    \coordinate (Dt) at (1.3,2.4);
    
    \draw[pile, white] (D) -- (Dt) node[right, text width=5em] {};
\end{tikzpicture}
\end{tabular}
\caption{Feature Mapping of a deep GP. Colors correspond to the location $\vy = \vf(\vx)$ that each point is mapped to after being warped by a deep GP.  %This figure can be seen as the inverse of figure \ref{fig:filamentation}.  
Just as the densities in figure \ref{fig:filamentation} became locally one-dimensional, there is usually only one direction that one can move $\vx$ in locally to change $\vy$.  The number of directions in which the color changes rapidly corresponds to the number of large singular values in the Jacobian.}
\label{fig:deep_map}
\end{figure}
%
To visualize this pathology in another way, figure \ref{fig:deep_map} illustrates the value that at each point in the input space is mapped to after successive warpings.  After 40 warpings, we can see that locally, there is usually only one direction that one can move in $\vx$-space in order to change the value of the function.

To what extent are these pathologies present in nets being used today?  In simulations, we found that for deep functions with a fixed latent dimension $D$, the singular value spectrum remained relatively flat for hundreds of layers as long as $D > 100$.  Thus, these pathologies are unlikely to severely affect relatively shallow, wide networks.




\section{Fixing the pathology}
\sectiondist
\label{sec:fix}

\begin{figure}[h!]
\input{tables/input-connected}
\caption{Two different architectures for deep neural networks.  The standard architecture connects each layer's outputs to the next layer's inputs.  The input-connected architecture connects also connects the original input $\vx$ to each layer.}
\label{fig:input-connected}
\end{figure}


Following a suggestion from \cite{neal1995bayesian}, we can fix the pathologies exhibited in figures \ref{fig:filamentation} and \ref{fig:deep_map} by simply making each layer depend not only on the output of the previous layer, but also on the original input $\vx$.  
We refer to these models as \emph{input-connected networks}.
Figure \ref{fig:input-connected} shows a graphical representation of the two connectivity architectures.
Similar connections between non-adjacent layers can also be found the primate visual cortex \citep{maunsell1983connections}.
Formally, %In input-connected networks, 
\begin{align}
\vf^{(1:L)}(\vx) = \vf^{(L)} \left( \vf^{(1:L-1)}(\vx), \vx \right), \quad \forall L
\end{align}
%
Draws from the resulting prior are shown in figures \ref{fig:deep_draw_1d_connected}, \ref{fig:no_filamentation} and \ref{fig:deep_map_connected}.
%
\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\hspace{-0.1in} One layer & \hspace{-0.2in} 2-Layer Composition \\
\hspace{0.03in}
\onedsamplepiccon{1} &
\onedsamplepiccon{2} \\
\hspace{-0.2in}  5-Layer Composition & \hspace{-0.25in} 10-Layer Composition \\
\onedsamplepiccon{5} &
\onedsamplepiccon{10}
%& 6 layers \\ & largest , for unconnected layers & largest, for connected layers
\end{tabular}
\caption{Draws from a 1D deep \gpt{} prior with each layer connected to the input. Even after many layers, the functions remain smooth in some regions, while varying rapidly in other regions.}
\label{fig:deep_draw_1d_connected}
\end{figure}
%
%Figure \ref{fig:deep_draw_1d_connected} shows draws from a 1D deep \gp{} prior with a connected architecture.
%
%\begin{figure}
%\centering
%\begin{tabular}{ccc}
%\includegraphics[width=0.3\columnwidth]{figures/spectrum/svd_specturm_depth_50_connected} &
%\includegraphics[width=0.3\columnwidth]{figures/spectrum/svd_ratio_depth_50} &
%\includegraphics[width=0.3\columnwidth]{figures/spectrum/svd_ratio_depth_50_connected} \\
%50 connected layers & smallest singular value divided by & 6 layers \\
% & largest , for unconnected layers & largest, for connected layers
%\end{tabular}
%\caption{Reparing the singular value distribution.  When each layer is connected to the original inputs, and the variance of derivatives is smaller than a threshold, then the ratio of the largest singular value to the smallest remains small.}
%\label{fig:deep_spectrum_fixed}
%\end{figure}
%
%
\newcommand{\gpdrawboxcon}[1]{
\setlength\fboxsep{0pt}
\hspace{-0.2in} 
\fbox{
\includegraphics[width=0.464\columnwidth]{figures/deep_draws_connected/deep_sample_connected_layer#1}
}}
%
\begin{figure}[h!]
\centering
\begin{tabular}{cc}
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws/deep_gp_sample_layer_1} &
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws_connected/deep_sample_connected_layer2} &
%\includegraphics[width=0.3\columnwidth]{figures/deep_draws_connected/deep_sample_connected_layer3} \\
%$p(\vx)$ & $p(f_1(\vx))$ & $p(f_2(f_1(\vx), \vx))$ \\ \\
%\gpdrawboxcon{2} &
\gpdrawboxcon{4} &
\gpdrawboxcon{5} \\
 4 Layers & 5 Layers
\end{tabular}
\caption{Left: Densities defined by a draw from a deep GP, with each layer connected to the input $\vx$.  As depth increases, the density becomes more complex without concentrating along filaments.}
\label{fig:no_filamentation}
\end{figure}
%
%\begin{tabular}{c}
%
%\gpdrawboxcon{6} \\
%$p(f_3(f_2(f_1(\vx),\vx),\vx))$ & $p(f_4(f_3(f_2(f_1(\vx), \vx),\vx),\vx))$ & $p(f_5(f_4(f_3(f_2(f_1(\vx),\vx),\vx),\vx),\vx))$
\begin{figure}[h!]
\centering
\newcommand{\spectrumpiccon}[1]{
%\hspace{-0.2in}
\includegraphics[trim=5mm 0mm 4mm 6.4mm, clip, width=0.5\columnwidth]{figures/spectrum/con-layer-#1}} 
%\begin{figure}[h]
%\centering
\begin{tabular}{ccc}
20 layers & 
%4 layers & 
50 layers \\
\hspace{-0.16in} \spectrumpiccon{20} &
%\spectrumpic{4} &
\hspace{-0.16in} \spectrumpiccon{50} 
%2 layers & 4 layers & 6 layers
\end{tabular}
\caption{The distribution of singular values drawn from a 5-dimensional input-connected deep \gpt{} prior 25 and 50 layers deep.  The singular values remain roughly the same scale as one another.}
\label{fig:good_spectrum}
\end{figure}
%
%
\begin{figure}[h!]
\centering
\begin{tabular}{cc}
Identity Map: $\vy = \vx$ & 2 connected Layers \\
\hspace{-0.07in} \mappic{0} & \mappiccon{2} \\
 10 Connected Layers & 40 Connected Layers \\
 \mappiccon{10} & \mappiccon{40}
\end{tabular}
\caption{Feature Mapping of a deep GP with each layer connected to the input $\vx$.  Just as the densities in figure \ref{fig:no_filamentation} remained locally two-dimensional even after many transformations, in this mapping there are usually two directions that one can move locally in $\vx$ to change $\vy$.}
\label{fig:deep_map_connected}
\end{figure}
%
%
%
%\paragraph{Jacobian of input-connected deep networks}
The Jacobian of the composed, input-connected deep function is defined by the recurrence:
%
\newcommand{\sbi}[2]{\left[ \! \begin{array}{c} #1 \\ #2 \end{array} \! \right]} 
%\newcommand{\sbi}[2]{\left[ #1 \quad \!\! #2 \right]} 
%\begin{align}
${J^{1:L}(\vx) = J^L \sbi{ J^{1:L-1}}{I_D}}$.
%\end{align}
%
%So the entire Jacobian has the form:
%
%\begin{align}
%J^{1:L}(x) = J^L \sbi{ J^{L-1} \sbi{ \dots J^{4} \sbi{ J^{3} \sbi{ J^2 J^1 }{ I_D }}{ I_D } \dots }{ I_D \\ %\vdots }}{ I_D}
%\end{align}
%
Figure \ref{fig:good_spectrum} shows that with this architecture, even 50-layer deep \gp{}s have well-behaved singular value spectra.

%Connecting the input to each layer is a simple method to correct the pathology exhibited, but it may not




\section{Deep kernels}
\label{sec:deep_kernels}
\sectiondist

\cite{NIPS2005_424} showed that kernel machines have limited generalization ability when they use a local kernel such as the squared-exp.
However, many interesting non-local kernels can be constructed which allow non-trivial extrapolation, for example, periodic kernels.
Periodic kernels can be viewed as a 2-layer-deep kernel, in which the first layer maps $x \rightarrow [\sin(x), \cos(x)]$, and the second layer maps through the basis functions corresponding to the \humble{SE} kernel.

%In addition to analyzing \MLP{}s with random weights, we can also analyze fixed feature mappings with different connectivity architectures.
 
Can we construct other useful kernels by composing fixed feature maps several times, creating deep kernels?  \citet{cho2012kernel} constructed kernels of this form, repeatedly applying multiple layers of feature mappings.
Given a kernel $k_1(\vx, \vx') = \hPhi(\vx) \tra \hPhi(\vx')$, we can compose its feature mapping:
\begin{align}
k_2(\vx, \vx') 
 = k_2 \left(\hPhi(\vx), \hPhi(\vx') \right)
 = \hPhi \left( \hPhi(\vx)) \tra \hPhi(\hPhi(\vx') \right) \nonumber
\end{align}

For the squared-exp kernel, this composition operation has a closed form:% for any set of starting features $\hPhi_n(\vx)$:
%
\begin{align*}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
& k_{L+1} \left( \vx, \vx' \right) = k_{SE} \left( \hPhi(\vx), \hPhi(\vx') \right) = \\
%& = \left( \hPhi^{SE} \left(\hPhi^{1}(\vx) \right) \right) \tra \hPhi^{SE} \left( \hPhi^{1}(\vx') \right) \\
& = \exp \left( -\frac{1}{2} || \hPhi(\vx) - \hPhi(\vx')||_2^2 \right) \nonumber\\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( -\frac{1}{2} \sum_i \left[ \hphi_n^{(i)}(\vx) - \hphi_n^{(i)}(\vx') \right]^2 \right) \\
& = \exp\left ( -\frac{1}{2} \left[ \hPhi(\vx) \tra \hPhi(\vx) - 2 \hPhi(\vx) \tra \hPhi(\vx') + \hPhi(\vx') \tra \hPhi(\vx') \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \hphi_i(\vx)^2 - 2 \sum_i \hphi_i(\vx) \hphi_i(\vx') + \sum_i \hphi_i(\vx')^2 \right] \right) \\
%k_{n+1}(\vx, \vx') 
& = \exp \left( -\frac{1}{2} \left[ k_{L}(\vx, \vx) - 2 k_{L}(\vx, \vx') + k_{L}(\vx', \vx') \right] \right)
%k_{n+1}(\vx, \vx') 
%& = \exp \left( k_1(\vx, \vx') - 1 \right) \qquad \textnormal{(if $k_1(\vx, \vx) = 1$)} \nonumber
\end{align*}
%
%\begin{figure}
%\centering
%\begin{tabular}{ccc}
%\includegraphics[width=0.5\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel} &
%\includegraphics[width=0.5\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel_draws} \\
%Kernel derived from iterated feature transforms & Draws from the corresponding kernel
%\end{tabular}
%\caption{A degenerate kernel produced by repeatedly applying a feature transform.}
%\label{fig:deep_kernel}
%\end{figure}
%
%Thus, if $k_1(x,y) = e^{-||x - y||2}$, then the two-layer kernel is simply $k_2(x,y) = e^{k_1(x, y) - 1}$.  This formula is true for every layer: $k_{n+1}(x,y) = e^{k_n(x, y) - 1}$.
%
%Note that nothing in this derivation depends on details of $k_n$, except that $k_n( \vx, \vx) = 1$.
%Note that this result holds for any base kernel $k_n$, as long as $k_n( \vx, \vx) = 1$.
%  Because this is true for $k_2$ as well, this recursion holds in general, and we have that $k_{n+1}(x,y) = e^{k_n(x, y) - 1}$.  
Thus, we can express $k_{L+1}$ exactly in terms of $k_L$.

\paragraph{Infinitely deep kernels}
What happens when repeat this feature mapping many times, starting with the squared-exp kernel?  In the infinite limit, this recursion converges to $k(\vx,\vx') = 1$ for all pairs of inputs, which is simply a prior on constant functions $f(\vx) = c$.

%Figure \ref{fig:deep_kernel_connected} shows this kernel at different depths, including the degenerate limit.  
%
%One interpretation of why repeated feature transforms lead to this degenerate prior is that each layer can only lose information about the previous set of features.  
%In the limit, the transformed features contain no information about the original input $\vx$.  Since the function doesn't depend on its input, it must be the same everywhere.

\paragraph{A non-degenerate construction}

As before, we can overcome this degeneracy by % following a suggestion from \cite{neal1995bayesian}, we 
connecting the inputs $\vx$ to each layer.  To do so, we simply augment the feature vector $\hPhi_n(\vx)$ with $\vx$ at each layer: 
%
\begin{align}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
 k_{L+1}(\vx, \vx') % = \nonumber\\
& = \exp \left( -\frac{1}{2} \left|\left| \left[ \! \begin{array}{c} \hPhi_L(\vx) \\ {\color{blue} \vx} \end{array} \! \right]  - \left[ \! \begin{array}{c} \hPhi_L(\vx') \\ {\color{blue} \vx'} \end{array} \! \right] \right| \right|_2^2 \right) \nonumber \\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( -\frac{1}{2} \sum_i \left[ \hphi_i(\vx) - \hphi_i(\vx') \right]^2 -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp\left ( -\frac{1}{2} \sum_i \left[ \hphi_i(\vx)^2 - 2 \hphi_i(\vx) \hphi_i(\vx') + \hphi_i(\vx')^2 \right]  -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \hphi_i(\vx)^2 - 2 \sum_i \hphi_i(\vx) \hphi_i(\vx') + \sum_i \hphi_i(\vx')^2 \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ k_1(\vx, \vx) - 2 k_1(\vx, \vx') + k_1(\vx', \vx') \right] \right) \\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( k_n(\vx, \vx') - 1 -\frac{1}{2} || \vx - \vx' ||_2^2 \right)
& = \exp \Big( -\frac{1}{2} \big[ k_{L}(\vx, \vx) - 2 k_{L}(\vx, \vx') \nonumber \\ 
& \qquad \qquad + k_{L}(\vx', \vx') {\color{blue} - || \vx - \vx' ||_2^2} \big] \Big)
\end{align}
%
For the \humble{SE} kernel, this repeated mapping satisfies
\begin{align}
k_\infty(\vx, \vx') - \log \left( k_\infty(\vx, \vx') \right) = 1 + \frac{1}{2} || \vx - \vx' ||_2^2
\end{align}
%
The solution to this recurrence has no closed form, but it has a similar shape to the Ornstein-Uhlenbeck covariance ${k_{\textnormal{OU}}(x,x') = \exp( -|x - x'| )}$, with lighter tails.
%
Samples from a \gp{} prior with this kernel are not differentiable, and are locally fractal.
%\item This kernel has smaller correlation than the squared-exp everywhere except at $\vx = \vx'$.  
%\item The tails have the same form as the squared-exp.

\begin{figure}[h]
\centering
\begin{tabular}{c}
%\hspace{-0.5cm}\includegraphics[width=0.35\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel} &
\hspace{-0.5cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.61cm]{figures/deep_kernel_connected}
%input-connected deep kernels
\end{tabular}
\caption{
A family of input-connected deep kernels.  By connecting the inputs $\vx$ to each layer, the function can still depend on its input even after arbitrarily many layers of computation.}
\label{fig:deep_kernel_connected}
\end{figure}


\begin{figure}[h]
\centering
\begin{tabular}{c}
%\hspace{-0.5cm}\includegraphics[width=0.35\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel} &
\hspace{-0.5cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.61cm]{figures/deep_kernel_connected_draws} \\
\end{tabular}
\caption{Draws from the deep input-connected kernel.  
}
\label{fig:deep_kernel_connected_draws}
\end{figure}



%\subsection{A fully-connected kernel}
%However, connecting every layer to every subsequent layer leades to a pathology:
%\begin{align}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp \left( -\frac{1}{2} \left|\left| \left[ \! \begin{array}{c} \hPhi_n(\vx) \\ \hPhi_{n-1}(\vx') \end{array} \! \right]  - \left[ \! \begin{array}{c} \hPhi_n(\vx') \\ \hPhi_{n-1}(\vx') \end{array} \! \right] \right| \right|_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp \left( -\frac{1}{2} \sum_i \left[ \hphi^{(i)}_n(\vx) - \hphi^{(i)}_n(\vx') \right]^2 -\frac{1}{2} \sum_i \left[ \hphi^{(i)}_{n-1}(\vx) - \hphi^{(i)}_{n-1}(\vx') \right]^2 \right)\\
%k_{n+1}(\vx, \vx') & = \exp\left ( -\frac{1}{2} \sum_i \left[ \hphi_i(\vx)^2 - 2 \hphi_i(\vx) \hphi_i(\vx') + \hphi_i(\vx')^2 \right]  -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \hphi_i(\vx)^2 - 2 \sum_i \hphi_i(\vx) \hphi_i(\vx') + \sum_i \hphi_i(\vx')^2 \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ k_1(\vx, \vx) - 2 k_1(\vx, \vx') + k_1(\vx', \vx') \right] \right) \\
%k_{n+1}(\vx, \vx') & = \exp \left( k_n(\vx, \vx') - 1 \right) \exp \left( k_{n-1}(\vx, \vx') - 1 \right)
%k_{n+1}(\vx, \vx') & = \prod_{i=1}^n \prod_{j=1}^i \exp \left( k_i(\vx, \vx') - 1 \right)
%\end{align}
%Which has the solution $k_\infty = \delta( \vx = \vx' )$, a white noise kernel.


%\subsection{Connecting every layer to the end}
%There is a fourth possibilty (suggested by Carl), of connecting every layer to the output:
%\begin{align}
%k_n(\vx, \vx') & = \exp \left( -\frac{1}{2} || \hPhi_n(\vx) - \hPhi_n(\vx')||_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp \left( k_n(\vx, \vx') - 1 \right) \\
%k_{L}(\vx, \vx') & = \prod_{i=1}^L \exp \left( k_i(\vx, \vx') - 1 \right)
%\end{align}
%Which also has the solution $k_\infty = \delta( \vx = \vx' )$, a white noise kernel.


\subsection{When are deep kernels useful models?}
\sectiondist
Kernels corresponding to fixed feature maps can compute %useful representations.
rich structure, and can enable many types of generalization, such as translation and rotation invariance in images \citep{kondor2008group}.
\cite{SalHin08} used a deep neural network to learn feature transforms for kernels, which learn invariances in an unsupervised manner.
%However, any fixed representation is unlikely to be useful for a given problem unless it has been optimized specifically for that problem.
The relatively uninteresting properties of the kernels derived in this section simply reflect the fact that an arbitrary deep computation is not usually a useful representation, unless combined with learning.



\section{Dropout in Gaussian processes}
\sectiondist


Dropout is a method for regularizing neural networks \citep{hinton2012improving, srivastava2013improving}.
Training with dropout entails randomly ``dropping'' (setting to zero) some proportion $p$ of features or inputs, in order to improve the robustness of the resulting network by reducing co-dependence between neurons.
To maintain similar overall activation levels, weights are multiplied by $\nicefrac{1}{p}$ at test time. Alternatively, feature activations are multiplied by $\nicefrac{1}{p}$ during training.
At test time, the set of models defined by all possible ways of dropping-out neurons is averaged over, usually in an approximate way.

\citet{baldi2013understanding} and \citet{wang2013fast} analyzed dropout in terms of the effective prior induced by this procedure in several models, such as linear and logistic regression.
In this section, we examine the priors on functions that result from performing dropout in the one-layer neural network implicitly defined by a \gp{} (equation \eqref{eq:one-layer-nn}).


\subsection{Dropout on feature activations}

First, we examine the prior that results from randomly dropping features from $\hPhi(x)$ with probability $p$.
If these features have a weight distribution with finite moments
$\expectargs{}{ \alpha_i} = \mu, \varianceargs{}{\alpha_i} = \sigma^2$,
then the distribution of weights after each one has been dropped out with probability $p$ is:
\begin{align}
r_i \simiid \textnormal{Ber}(p)
\qquad
\expectargs{}{ r_i \alpha_i} = p\mu, \varianceargs{}{r_i \alpha_i} = p^2 \sigma^2
\end{align}
However, after we increase the remaining activations to maintain the same expected activation, (by multiplying them by $\nicefrac{1}{p}$), the resulting moments are again:
\begin{align}
{\expectargs{}{ \nicefrac{p}{p} r_i \alpha_i } = \mu, \varianceargs{}{ \nicefrac{p}{p} r_i \alpha_i} = \sigma^2}.
\end{align}
%Since only the first two moments of the weight variance matter, the resulting \gp{} remain
Thus, dropping out features of an infinitely-wide MLP does not change the model at all, since no individual feature can have more than infintesimal contribution to the activations.
%Is there a better way to drop out features that would lead to robustness?

\subsection{Dropping out inputs}

In a \gp{} with a kernel ${k(\vx, \vx') = \prod_{d=1}^D k_d(\vx_d, \vx_d')}$, exact averaging over all possible ways of dropping out inputs with probability $\nicefrac{1}{2}$ results in a mixture of \gp{}s, each depending on only a subset of the inputs:
\begin{align}
p \left( f(\vx) \right)= \frac{1}{2^D} \sum_{\vr \in \{0,1\}^D}  \textnormal{\gp} \left(0, \prod_{d=1}^D k_d(\vx_d, \vx_d')^{r_d} \right)
\end{align}
%Probably a nice model, but presumably intractable.
For an \humble{SE} kernel, this model corresponds to a spike-and-slab prior on the inverse lengthscales (relevances).
%In neural net literature, exponentially-many dropout models are averaged over in tractable ways.

This dropout mixture has the same covariance as
\begin{align}
f(\vx) \sim \textnormal{\gp} \left(0, \frac{1}{2^D} \sum_{\vr \in \{0,1\}^D}  \prod_{d=1}^D k_d(\vx_d, \vx_d')^{r_d} \right).
\label{eq:additive-gps}
\end{align}

In \gp{} models, a sum of covariance functions corresponds to a sum of functions.  Therefore, \eqref{eq:additive-gps} describes a sum of $2^D$ functions, each depending on a different subset of the inputs.
This model class was studied by \citet{duvenaud2011additive11}, who showed that exact inference in these models can be performed in $\mathcal{O}(N^2 D^2) + \mathcal{O}(N^3)$.
%Thus, the dropout prior is closely related to a model which learns many functions, each of which only depends on a subset of the inputs.

\section{Related work}
\sectiondist

Deep \gp{}s were first proposed by \cite{lawrence2007hierarchical}. Variational inference in deep \gp{}s was developed by \cite{damianou2012deep}, who also analyzed the effect of automatic relevance determination in that model.

%Other Bayesian deep neural network models have been proposed by, for example, . % and Sum-product networks \cite{poon2011sum}  
\citet{adams2010learning} proposed a prior on deep Bayesian networks.  Their architecture has no connections except between adjacent layers, and may also be expected to have similar pathologies as deep \gp{}s as the number of layers increases.
%
Deep Density Networks \citep{rippel2013high} were constructed with invertibility in mind, with penalty terms encouraging the preservation of information about lower layers. Such priors are a promising approach to alleviating the pathology discussed in this paper.

\paragraph{Recurrent networks}
\cite{bengio1994learning} and \cite{pascanu2012understanding} analyze a related problem with gradient-based learning in recurrent nets, the ``exploding-gradients'' problem.
\cite{hermans2012recurrent} analyze deep kernels corresponding to recurrent neural networks.

\paragraph{Analysis of deep learning}
\cite{montavon2010layer} perform a layer-wise analysis of deep networks, and note that the performance of MLPs degrades as the number of layers with random weights increases.
The importance of network architectures relative to learning has been examined by \cite{saxe2011random}.
\cite{saxedynamics} also looked at learning dynamics in deep linear models.  




%\section{Discussion}


%\paragraph{Recursive learning method}
%Just as layer-wise unsupervised pre-training encourages the projection of the data into a representation with independent features in the higher layers, so does the procedure outlined here.  This is because the isotropic kernel does not penalize independence between different dimensions, only the number of dimensions.





\section{Conclusions}
\sectiondist

In this work, we established a number of propositions which help us gain insight into the properties of very deep models, and allow making informed choices regarding their architecture.

First, we identified a strong equivalence between deep \gp{}s and \MLP{}s --- namely, that deep \gp{}s can be written as \MLP{}s with a finite number of nonparametric hidden units. 

Second, we showed that deep \gp{}s can be characterized using random matrix theory, which we applied to establish results regarding the distribution over the Jacobian of the composition transformation.

Next, we demonstrated that representations based on repeated composition of independently-initialized functions exhibit a pathology where the representations becomes invariant to all directions of variation, but one. This leads to extremely restricted expressiveness of such deep models in the limit of increasing number of layers. 

Finally, we proposed a way to alleviate this problem: connecting the input to each layer of a deep representation allows us to construct priors on deep functions that do not exhibit the information-capacity pathology.


\subsubsection*{Acknowledgements}
We thank Carl Rasmussen, Andrew McHutchon, Neil Lawrence, Andreas Damianou, James Lloyd, Creighton Heaukulani, Dan Roy and Mark van der Wilk for helpful discussions.

\subsubsection*{References}
\vspace{-0.3in}
\renewcommand{\refname}{}

\bibliographystyle{include/natbib}
\bibliography{verydeep}


\end{document}




%
\begin{align}
\expectargs{}{ 2r_i \alpha_i} = 0, \quad \varianceargs{}{{\color{green}2}r_i \alpha_i} = 
{\color{green}\frac{4}{4}}\sigma^2
\end{align}

\begin{align}
\cov \left[ \! \begin{array}{c} f(\vx) \\ f(\vx') \end{array} \! \right] \to 
{\color{green}\frac{4}{4}}
\frac{\sigma^2}{K}\sum_{i=1}^K \hphi_i(\vx)\hphi_i(\vx')
\end{align}

If we propose augmenting \eqref{eq:one-layer-nn} with random bernoulli variables,
%
\begin{align}
f(\vx) = \frac{1}{2}{K} \sum_{i=1}^K {\color{red}r_i} \alpha_i \hphi_i(\vx) \quad {\color{red}r_i} \simiid \textnormal{Ber}(p)
\end{align}


by CLT, gives a GP as $K \to \infty$
\begin{align}
\cov \left[ \! \begin{array}{c} f(\vx) \\ f(\vx') \end{array} \! \right] \to 
{\color{green}\frac{4}{4}}
\frac{\sigma^2}{K}\sum_{i=1}^K \hphi_i(\vx)\hphi_i(\vx')
\end{align}


