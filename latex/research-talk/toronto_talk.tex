\input{header_beamer}
\usepackage{etex}
%\include{macros}
%\documentclass[usenames,dvipsnames]{beamer}
%\usepackage{beamerthemesplit}
%\usepackage{graphics}
%\usepackage{amsmath}
%\usepackage{rotating}
%\usepackage{array}
%\usepackage{nth}
\usepackage{xcolor}
\usepackage{textcomp}
\input{matlab_setup}

\usepackage{tabularx}
\usepackage{picins}
\usepackage{tikz}
\usepackage{changepage}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]

\definecolor{camlightblue}{rgb}{0.601 , 0.8, 1}
\definecolor{camdarkblue}{rgb}{0, 0.203, 0.402}
\definecolor{camred}{rgb}{1, 0.203, 0}
\definecolor{camyellow}{rgb}{1, 0.8, 0}
\definecolor{lightblue}{rgb}{0, 0, 0.80}
\definecolor{white}{rgb}{1, 1, 1}
\definecolor{whiteblue}{rgb}{0.80, 0.80, 1}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\tabbox}[1]{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Some look and feel definitions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\columnsep}{0.03\textwidth}
\setlength{\columnseprule}{0.0018\textwidth}
\setlength{\parindent}{0.0cm}

%\include{macros}
\usepackage{preamble}
\hypersetup{colorlinks=true,citecolor=blue}
%\pdfmapfile{+sansmathaccent.map}

\title{Automatic construction and description of nonparametric models
}

\author{
\includegraphics[height=0.2\textwidth, trim=20mm 25mm 0mm 25mm, clip]{figures/david2}
\qquad
\includegraphics[height=0.2\textwidth]{figures/JamesLloyd4}
\qquad
\includegraphics[height=0.2\textwidth]{figures/roger-photo}
\\
David Duvenaud, James Robert Lloyd, Roger Grosse,\\ Joshua Tenenbaum, Zoubin Ghahramani
}
\institute{
%\includegraphics[width=0.4\textwidth]{figures/spiral_main}
}
%\date{}


\begin{document}

\frame[plain] {
\titlepage
}

\setbeamercolor{toc}{fg=black}

%\frame[plain] {
%\frametitle{Outline}
%\tableofcontents

%\begin{itemize} 
%	\item Motivation
%	\item Automated structure discovery in regression
%	\begin{itemize} 
%		\item Gaussian process regression
%		\item Structures expressible through kernel composition
%		\item A massive missing piece
%		\item grammar \& search over models
%		\item Examples of structures discovered
%	\end{itemize}
%	\item Automated structure discovery in matrix models
%	\begin{itemize} 
%		\item expressing models as matrix decompositions
%		\item grammar \& special cases
%		\item examples of structures discovered on images
%	\end{itemize}
%\end{itemize}   
%
%}


%\frame[plain]{
%\frametitle{Credit where credit is due}
%
%Talk based on two papers:
%	\begin{itemize}
%		\item Structure Discovery in Nonparametric Regression through Compositional Kernel Search [ICML 2013]
%		\\
%		{David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani}
%		\item Exploiting compositionality to explore a large space of model structures [UAI 2012]
%		\\
%		Roger B. Grosse, Ruslan Salakhutdinov, \\William T. Freeman, Joshua B. Tenenbaum
%	\end{itemize}
%}


\frame[plain]{
\frametitle{Typical statistical modelling}
\begin{itemize} 
	\item models typically built by hand, or chosen from a fixed set
	\begin{center}
		\includegraphics[width=9cm, trim=1.39cm 15cm 35cm 0cm, clip]{figures/plot_classifier_comparison_1}\\
		\includegraphics[width=9cm, trim=35cm 15cm 1.35cm 0cm, clip]{figures/plot_classifier_comparison_1}
\end{center}
%	\begin{itemize} 
%	  \vspace{\baselineskip}
%		\item Example: Scikit-learn
			\item Building by hand requires considerable expertise% and understanding of the dataset 
%	\begin{itemize}
%	  \item Can become an entire research project
%	\end{itemize}
%	\end{itemize}
%	\vspace{\baselineskip}
	\item Just being nonparametric isn't good enough
	\begin{itemize}
	  \item Nonparametric does not mean assumption-free!
	\end{itemize}
%	\vspace{\baselineskip}
	\item Can silently fail
	\begin{itemize}
	  \item If none of the models tried fit the data well, how can you tell?
	\end{itemize}
\end{itemize}
}








\frame[plain]{
\frametitle{Can we do better?}
	\begin{itemize} 
	\item Andrew Gelman asks:  How can an AI do statistics?
	\vspace{\baselineskip}
	\item An artificial statistician would need:
			\begin{itemize} 
			\item a language that could describe arbitrarily complicated models
			\item a method of searching over those models
			\item a procedure to check model fit
		\end{itemize}
	\vspace{\baselineskip}
	\item This talk:We construct such a language over regression models, a procedure to search over it, and a method to describe in natural language the properties of the resulting models
	\begin{itemize}
		\item Working on automatic model-checking\ldots
	\end{itemize}
\end{itemize}
}


\definecolor{verylightblue}{rgb}{0.97,0.97,1}
\setlength{\fboxsep}{0pt}

\newcommand{\ltrim}{ 2 }
\newcommand{\rictrim}{ 2 }
%\newcommand{\airlinefig}[1]{\includegraphics[trim=20 0 12 20, clip, width=0.207\textwidth]{figures/#1}}
%\newcommand{\airlinefigtwo}[1]{}
\newcommand{\olduptext}[1]{\hspace{-1cm} \raisebox{ 0.8cm}{ {#1}} \hspace{-0.75cm} }
\newcommand{\uptext}[1]{\raisebox{1cm}{#1}}

\frame[plain]{
\frametitle{Example: An automatic analysis}


\begin{adjustwidth}{-1.2cm}{}
\begin{tabular}{@{}c@{}}
%\airlinefig{01-airline-months_all}&\hspace{0.6cm}\olduptext{$=$} \hspace{-0.1cm}
%\airlinefig{01-airline-months_1} & \olduptext{$+$}
%\airlinefig{01-airline-months_2} & \olduptext{$+$}
%\airlinefig{/01-airline-months_3} \\
%\hspace{-3.5mm}
%\fbox{

%\hspace{-3.5mm}
\begin{tabular}{@{}cc@{}}
%\\[-0.7em]
%\fcolorbox{blue}{white}{
\includegraphics[trim=7.8cm 14.5cm 0cm 2cm, clip, width=0.5\columnwidth]{figures/airline-pages/pg_0002-crop} 
%}
 & \uptext{$=$} \\
%{\scriptsize Original data} & 
\end{tabular}

\\
\begin{tabular}{p{3.45cm}p{0.3cm}p{3.45cm}p{0.3cm}p{3.45cm}}
\\%[1cm]
\includegraphics[trim=0.4cm 6cm 8.4cm 2.75cm, clip, width=0.33\columnwidth]{figures/airline-pages/pg_0003-crop} & 
 \uptext{$+$}  &  
\includegraphics[trim=0.4cm 6cm 8.4cm 2.88cm, clip, width=0.33\columnwidth]{figures/airline-pages/pg_0004-crop} & 
\uptext{$+$} &  
\includegraphics[trim=0.4cm 6cm 8.4cm 2.75cm, clip, width=0.33\columnwidth]{figures/airline-pages/pg_0005-crop} \\
{\scriptsize A very smooth, monotonically increasing function }
& & 
{\scriptsize An approximately periodic function with a period of 1.0 years and with
approximately linearly increasing amplitude}
& & 
{\scriptsize An exactly periodic function with a period of 4.3 years but with linearly
increasing amplitude }
\end{tabular}
\end{tabular}

\end{adjustwidth}
}




\frame[plain]{
\frametitle{A language of regression models}
\begin{itemize} 
	\item We define a language of Gaussian process (GP) regression models by defining a language over kernel functions
	\vspace{\baselineskip}
	\item We start with a small set of base kernels and create a language with a generative grammar
		\begin{itemize}
		\item $ K \rightarrow K + K$ 
		\item $ K \rightarrow K \times K$ 
		\item $ K \rightarrow CP( K, K)$ 
		\item $ K \rightarrow \{ \SE, \Lin, \Per \}$
	\end{itemize}
%	\begin{itemize}
%	  \item Expansion rules include addition, multiplication and change-points
%	\end{itemize}
	\vspace{\baselineskip}
	\item The language is open-ended, but its structure makes natural-language description simple
\end{itemize}
}


\frame[plain]{
\frametitle{Kernels determine structure of GPs}
\begin{itemize} 
	\item Kernel determines almost all the properties of a GP prior
	\item Many different kinds, with very different properties:
\end{itemize}
\input{tables/simple_kernels_table_v4}
}


\frame[plain]{
\frametitle{Kernels can be composed}
\begin{itemize} 
	\item Two main operations: addition, multiplication
\end{itemize}
\input{tables/comp1v2}
}

%\frame[plain]{
%\frametitle{Kernels can be composed}
%\begin{itemize} 
%	\item Can be composed across multiple dimensions
%\end{itemize}
%\input{tables/comp2v2}
%}



\frame[plain]{
\frametitle{Special cases in our language}
%\begin{center}
%  \begin{tabular}{l|l}
%  Bayesian linear regression & $\Lin$ \\
%  %Bayesian quadratric regression & $\Lin \times \Lin$ \\
%  Bayesian polynomial regression & $\Lin \times \Lin \times \ldots$\\
%  Generalized Fourier decomposition & $\Per + \Per + \ldots$ \\
%  Generalized additive models & $\sum_{d=1}^D \SE_d$ \\
%  Automatic relevance determination & $\prod_{d=1}^D \SE_d$ \\
%  Linear trend with deviations & $\Lin + \SE$ \\
%  Linearly growing amplitude & $\Lin \times \SE$
%  \end{tabular}
%\end{center}
\begin{center}
\begin{tabular}{l|l}
Regression motif & Example kernel \\
\midrule
Linear regression & $\kLin$ \\
Quadratric regression & $\Lin \times \Lin$ \\
Fourier analysis & $\sum \cos$ \\
%Sparse spectrum \gp{}s & $\sum \cos$ \\
Spectral kernels & $\sum \SE \times \cos$ \\
Changepoints & \eg $\kCP(\kPer, \kSE)$ \\
%Kernel smoothing & $\kSE$ \\
Heteroscedasticity & \eg $\kSE + \kLin \times \kSE$ \\
%irregular Trend cyclical & $\sum \kSE + \sum \kPer$ \\
%Additive nonparametric modelling & $\sum \kSE$ \\
\end{tabular}
\end{center}
}


%\frame[plain]{
%\frametitle{Compositional structure search}
%\begin{itemize}
%	\item Define grammar over kernels:

%	\vspace{\baselineskip}
%	\item Search the space of kernels greedily by applying local search operators, maximising approximate marginal likelihood.
%\end{itemize}
%}




\tikzset{hide on/.code={\only<#1>{\color{white}}}}

\frame[plain]{
\frametitle{Compositional Structure Search}
\hspace{-1.2cm}
\only<1>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_0/03-mauna2003-s_all_small.pdf}}
\only<2>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_1/03-mauna2003-s_all_small.pdf}}
\only<3>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_2/03-mauna2003-s_all_small.pdf}}
\only<4>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_3/03-mauna2003-s_all_small.pdf}}

\vspace{-3.5cm}
\begin{minipage}[t][14cm][t]{1.14\linewidth}
\begin{flushleft}
\hspace{5.5cm}
\vspace{-8cm}
\makebox[\textwidth][c]{
\raisebox{10cm}{
\vspace{-8cm}
\begin{tikzpicture}
[sibling distance=0.18\columnwidth,-,thick, level distance=0.13\columnwidth]
%\footnotesize
\node[shape=rectangle,draw,thick] {Start}
%\pause
  child {node {$\SE$}}
%  fill=camlightblue!30
  child {node[shape=rectangle,draw,thick] {$\RQ$}
    [sibling distance=0.16\columnwidth]
%    {\visible<2->{ child {node {\ldots}}}}
    child [hide on=-1] {node {$\SE$ + \RQ}}
    child [hide on=-1] {node {\ldots}}
    child [hide on=-1] {node[shape=rectangle,draw,thick] {$\Per + \RQ$}
      [sibling distance=0.23\columnwidth]
      child [hide on=-2] {node {$\SE + \Per + \RQ$}}
      child [hide on=-2] {node {\ldots}}
      child [hide on=-2] {node[shape=rectangle,draw,thick] {$\SE \times (\Per + \RQ)$}
        [sibling distance=0.14\columnwidth]
        child [hide on=-3] {node {\ldots}}
        child [hide on=-3] {node {\ldots}}
        child [hide on=-3] {node {\ldots}}
      }
      child [hide on=-2] {node {\ldots}}
    }
    %child {node {$\RQ \times \SE$}}
    child [hide on=-1] {node {\ldots}}
    child [hide on=-1] {node {$\Per \times \RQ$}}
  }
  child {node {$\Lin$}}
  child {node {$\Per$}}
  ;
\end{tikzpicture}}
}\end{flushleft}
\end{minipage}
\only<4>{}
}



\frame[plain]{
\frametitle{Distributivity helps Interpretability}

We can write all kernels as sums of products of base kernels:
$${\SE \times (\RQ + \Lin) = (\SE \times \RQ) + (\SE \times \Lin)}.$$

Sums of kernels are equivalent to sums of functions.

\vspace{\baselineskip}

If $f_1, f_2$ are independent, and ${f_1 \sim \GP(\mu_1, k_1)}$, ${f_2 \sim \GP(\mu_2, k_2)}$ then $${(f_1 + f_2) \sim \GP(\mu_1 + \mu_2, k_1 + k_2)}$$
}

\frame[plain]{
\frametitle{Example Decomposition: Airline }
\begin{center}
  \input{figures/fig_airline_text_kern.tex}
\end{center}
}


\frame[plain]{
\frametitle{Describing Kernels}

Products of same type of kernel collapse.
\vspace{0.5cm}

\centering
\begin{tabular}{l|l}
Product of Kernels & Becomes \\
\midrule
$\kSE \times \kSE \times \kSE \dots$ & \kSE \\
$\kLin \times \kLin \times \Lin \dots$ & A polynomial \\
$\kPer \times \kPer \times \kPer$ & Same covariance as \\ & product of periodic functions
\end{tabular}
}


\frame[plain]{
\frametitle{Describing Kernels}

Each kernel acts as a modifier in a standard way: an ``adjective''.
\vspace{0.5cm}

\begin{tabular}{l|l}
Kernel & Becomes \\
\midrule
$K \times \kSE$ & 'locally' or 'approximately' \\
$K \times \kLin$ & 'with linearly growing amplitude' \\
$K \times \kPer$ & 'periodic' \\
$\kCP(K1, K2)$ & '\dots changing at $x$ to \dots' \\
\end{tabular}

\vspace{0.5cm}
\begin{itemize}
\item Special cases for when they're on their own
\item Extra adjectives depending on hyperparameters
\end{itemize}
}


\frame[plain]{
\frametitle{Example Kernel Descriptions}
\centering
\begin{tabular}{l|l}
Product of Kernels & Description \\
\midrule
$\kPer$ & An exactly periodic function \\
$\kPer \times \kSE$ & An approximately periodic function \\
$\kPer \times \kSE \times \kLin$ & An approximately periodic function \\ &  with linearly varying amplitude \\
%$\kLin$ & A linear function \\
%$\kLin \times \kLin$ & A quadratic function \\
%$\kPer \times \kLin \times \kLin$ & An exactly periodic function \\ & with quadratically varying amplitude\\
\end{tabular}
}


\frame[plain]{
\frametitle{This analysis was automatically generated}

\vspace{0.5\baselineskip}

\begin{center}
\fbox{\includegraphics[trim=0cm 9.5cm 0cm 0.7cm, clip, width=0.98\columnwidth]{figures/airline-pages/pg_0002-crop.pdf}}


\end{center}
}

\frame[plain]{
\frametitle{This analysis was automatically generated}

\begin{center}
\vspace{0.5\baselineskip}

\only<1>{
\fbox{\includegraphics[trim=0cm 6cm 0cm 0.9cm, clip, width=0.98\columnwidth]{figures/airline-pages/pg_0003-crop.pdf}}}

\only<2>{\fbox{\includegraphics[trim=0cm 6cm 0cm 0.0cm, clip, width=0.98\columnwidth]{figures/airline-pages/pg_0004-crop.pdf}}}

\only<3>{
\fbox{\includegraphics[trim=0cm 6cm 0cm 0.0cm, clip, width=0.98\columnwidth]{figures/airline-pages/pg_0005-crop.pdf}}}

\end{center}
}






\frame[plain]{
\frametitle{Summary}
\begin{itemize}
	\item Constructed a language of regression models through kernel composition
%	\vspace{\baselineskip}
	\item Searched over this language greedily
%	\vspace{\baselineskip}
	\item Kernels sums and products modify prior in predictable ways, allowing automatic natural-language description of models
%	\vspace{\baselineskip}
	\item Open questions:
	\begin{itemize}
		\item Interpretability versus flexibility
		\item Automatic Model-checking
	\end{itemize}
\end{itemize}
}



\frame[plain]{
\frametitle{Kernel learning as feature learning}
\begin{itemize}
	\item Kernels implicitly compute dot product between features of two items: $k(x,x') = \phi(x)\tra \phi(x')$.
%	\vspace{\baselineskip}
	\item one-to-one mapping between kernels and feature maps
	\item feature learning with tractable marginal likelihood!
	\vspace{\baselineskip}
	\item Can compose feature maps:
	\item exaxmple: periodic kernel $k_{per}(x,x') = \exp( - \sin^2(x - x') )$ is equiavelent to $k_{se}(\sin(x), \cos(x), \sin(x'), \cos(x')$.
%	\vspace{\baselineskip}
	\item What can we do with feature compositions?
\end{itemize}
}



\frame[plain]{
\frametitle{Deep Kernels}
\begin{itemize}
	\item (Cho, 2012) built kernels from multiple layers of feature mappings.
	\item given $k_1(\vx, \vx') = \Phi(\vx) \tra \Phi(\vx')$, we can consider $k_2(\vx, \vx') = \Phi(\Phi(\vx)) \tra \Phi(\Phi(\vx'))$.  For SE kernel:
%
\begin{align}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
& k_2(\vx, \vx') = \nonumber \\
& = \exp \left( -\frac{1}{2} || \Phi(\vx) - \Phi(\vx')||_2^2 \right) \nonumber\\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( -\frac{1}{2} \sum_i \left[ \phi_n^{(i)}(\vx) - \phi_n^{(i)}(\vx') \right]^2 \right) \\
%k_2(\vx, \vx') & = \exp\left ( -\frac{1}{2} \sum_i \left[ \phi_i(\vx)^2 - 2 \phi_i(\vx) \phi_i(\vx') + \phi_i(\vx')^2 \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \phi_i(\vx)^2 - 2 \sum_i \phi_i(\vx) \phi_i(\vx') + \sum_i \phi_i(\vx')^2 \right] \right) \\
%k_{n+1}(\vx, \vx') 
& = \exp \left( -\frac{1}{2} \left[ k_1(\vx, \vx) - 2 k_1(\vx, \vx') + k_1(\vx', \vx') \right] \right) \nonumber\\
%k_{n+1}(\vx, \vx') 
& = \exp \left( k_1(\vx, \vx') - 1 \right) \qquad \textnormal{(if $k_1(\vx, \vx) = 1$)} \nonumber
\end{align}
%
\item Why not go deeper?
\end{itemize}
}


\frame[plain]{
\frametitle{Infinitely Deep Kernels}
\begin{itemize}
	\item For SE kernel, $k_{n+1}(\vx, \vx') = \exp \left( k_1(\vx, \vx') - 1 \right)$.
	\item What is the eventual limit?
\end{itemize}
\centering
\begin{tabular}{cc}
\includegraphics[width=0.55\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel} &
\hspace{-1cm}\includegraphics[width=0.55\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel_draws} \\
Kernel & Draws from GP prior
\end{tabular}

\begin{itemize}
	\item $k_\infty(\vx, \vx') = 1$ everywhere.
\end{itemize}
}



\frame[plain]{
\frametitle{A simple fix...}
\begin{itemize}
	\item Following a suggestion from (Neal, 1995), we 
connect the inputs $\vx$ to each layer.  (append x to features at each layer:
%
\begin{align}
%k_1(\vx, \vx') & = \exp \left( -\frac{1}{2} ||\vx - \vx'||_2^2 \right) \\
& k_{n+1}(\vx, \vx') = \nonumber \\
& = \exp \left( -\frac{1}{2} \left|\left| \left[ \! \begin{array}{c} \Phi_n(\vx) \\ \vx \end{array} \! \right]  - \left[ \! \begin{array}{c} \Phi_n(\vx') \\ \vx' \end{array} \! \right] \right| \right|_2^2 \right) \nonumber \\
%k_{n+1}(\vx, \vx') 
%& = \exp \left( -\frac{1}{2} \sum_i \left[ \phi_i(\vx) - \phi_i(\vx') \right]^2 -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_{n+1}(\vx, \vx') & = \exp\left ( -\frac{1}{2} \sum_i \left[ \phi_i(\vx)^2 - 2 \phi_i(\vx) \phi_i(\vx') + \phi_i(\vx')^2 \right]  -\frac{1}{2} || \vx - \vx' ||_2^2 \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ \sum_i \phi_i(\vx)^2 - 2 \sum_i \phi_i(\vx) \phi_i(\vx') + \sum_i \phi_i(\vx')^2 \right] \right) \\
%k_2(\vx, \vx') & = \exp \left( -\frac{1}{2} \left[ k_1(\vx, \vx) - 2 k_1(\vx, \vx') + k_1(\vx', \vx') \right] \right) \\
%k_{n+1}(\vx, \vx') 
& = \exp \left( k_n(\vx, \vx') - 1 -\frac{1}{2} || \vx - \vx' ||_2^2 \right)
\end{align}
\end{itemize}
}


\frame[plain]{
\frametitle{Infinitely Deep Kernels}
\begin{itemize}
	\item $k_{n+1}(\vx, \vx') = \exp \left( k_n(\vx, \vx') - 1 -\frac{1}{2} || \vx - \vx' ||_2^2 \right)$.
	\item What is the eventual limit?
\end{itemize}
\centering
\begin{tabular}{cc}
\includegraphics[width=0.56\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel_connected} &
\hspace{-1cm}\includegraphics[width=0.55\columnwidth, clip, trim = 0cm 0cm 0cm 0.61cm]{figures/deep_kernel_connected_draws} \\
Kernel & Draws from GP prior
\end{tabular}

\begin{itemize}
	\item Looks like an OU process with skinny tails, samples are non-differentiable (fractal).
\end{itemize}
}


\frame[plain]{
\frametitle{What went wrong?}
\begin{itemize}
	\item Fixed feature mappings
	\item not capturing invariances
	\item not throwing away unnecessary information
	\item an illustration of why learning is usually necessary
\end{itemize}
}



\def\ie{i.e.\ }
\def\eg{e.g.\ }
\def\iid{i.i.d.\ }
%\def\simiid{\sim_{\mbox{\tiny iid}}}
\def\simiid{\overset{\mbox{\tiny iid}}{\sim}}
\def\simind{\overset{\mbox{\tiny \textnormal{ind}}}{\sim}}
\def\eqdist{\stackrel{\mbox{\tiny d}}{=}}
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}
%\newcommand{\vf}{\vect{f}}
\newcommand{\GPt}[2]{\mathcal{GP}\!\left(#1,#2\right)}

\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item a prior over compositions of functions:
	\begin{align}
\vf^{(1:L)}(\vx) = \vf^{(L)}(\vf^{(L-1)}(\dots \vf^{(2)}(\vf^{(1)}(\vx)) \dots))
\end{align}
%
where each $\vf_d^{(\ell)} \simind \GPt{0}{k^\ell_d(\vx, \vx')}$. 
	\item Can be seen as a certain limit of infinitely-wide deep nets.
	\item inference is really hard.	
	\item maybe we can learn something about deep models just from looking at prior draws?
\end{itemize}
}

\newcommand{\onedsamplepic}[1]{
\includegraphics[width=0.7\columnwidth]{figures/1d_samples/latent_seed_0_1d_large/layer-#1}}

\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item Draws from one-dimensional deep GPs:
	\vspace{\baselineskip}
	\only<1>{\onedsamplepic{1}}
	\only<2>{\onedsamplepic{2}}
	\only<3>{\onedsamplepic{3}}
	\only<4>{\onedsamplepic{4}}
	\only<5>{\onedsamplepic{5}}
	\only<6>{\onedsamplepic{6}}
	\only<7>{\onedsamplepic{7}}
	\only<8>{\onedsamplepic{8}}
	\only<9>{\onedsamplepic{9}}
	\only<10>{\onedsamplepic{10}}
\end{itemize}
}


\newcommand{\gpdrawbox}[1]{
\setlength\fboxsep{0pt}
\fbox{
\includegraphics[width=0.67\columnwidth]{figures/deep_draws/deep_gp_sample_layer_#1}
}}

\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item 2D to 2D warpings of a Gaussian density:
	\vspace{\baselineskip}
	\only<1>{\gpdrawbox{1}}
	\only<2>{\gpdrawbox{2}}
	\only<3>{\gpdrawbox{3}}
	\only<4>{\gpdrawbox{4}}
	\only<5>{\gpdrawbox{5}}
	\only<6>{\gpdrawbox{6}}
\end{itemize}
}



\newcommand{\mappic}[1]{\includegraphics[width=0.6\columnwidth]{figures/map/latent_coord_map_layer_#1}} 
\newcommand{\mappiccon}[1]{\includegraphics[width=0.6\columnwidth]{figures/map_connected/latent_coord_map_layer_#1}}


\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item Showing the x that gave rise to a particular y
	\item (i.e. decision boundaries) \\
	\vspace{\baselineskip}
	\only<1>{\mappic{0} \quad No warping}
	\only<2>{\mappic{1} \quad One Layers}
	\only<3>{\mappic{2} \quad Two Layers}
	\only<4>{\mappic{3} \quad Three Layers}
	\only<5>{\mappic{4} \quad Four Layers}
	\only<6>{\mappic{5} \quad Five Layers}
	\only<7>{\mappic{10} \quad Ten Layers}
	\only<8>{\mappic{20} \quad Twenty Layers}
	\only<9>{\mappic{40} \quad Forty Layers}
\end{itemize}
}

\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item Again following Radford's thesis, connect every layer to input:
	 \input{tables/input-connected}
\end{itemize}
}


\frame[plain]{
\frametitle{Deep Gaussian Processes}
\begin{itemize}
	\item Showing the x that gave rise to a particular y
	\item (i.e. decision boundaries) \\
	\vspace{\baselineskip}
	\only<1>{\mappic{0} \quad No warping}
	\only<2>{\mappiccon{2} \quad Two Layers}
	\only<3>{\mappiccon{10} \quad Ten Layers}
	\only<4>{\mappiccon{20} \quad Twenty Layers}
	\only<5>{\mappiccon{40} \quad Forty Layers}
\end{itemize}
}


\frame[plain]{
\frametitle{Summary}
\begin{itemize}
	\item GPs let us compute marginal likelihood, which enables model search
%	\vspace{\baselineskip}
	\item Kernel learning is an example of representation learning
%	\vspace{\baselineskip}
	\item How to build priors over functions that put mass on the sorts of structures we'd like to be able to learn?
	\item This question might shed light on good architectures or initialization strategies in a data-independent way.
%	\vspace{\baselineskip}
	\item Open questions:
	\begin{itemize}
		\item When is discrete kernel search a good way to find representations?
		\item What could we do if we could compute the marginal likelihood of a neural net?
	\end{itemize}
\end{itemize}
	\pause
	\centering
	{
		\hfill
		Thanks!
				\hfill
	}
}


\end{document}